# FA_768の方
import os
import glob
import csv
import random
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# 保存先ディレクトリ
labels_output_dir = "labels_output"
os.makedirs(labels_output_dir, exist_ok=True)

# rinna_F_A ディレクトリ
fix_F_A_dir = "rinna_F_A"
rinna_files = glob.glob(os.path.join(fix_F_A_dir, "*.csv"))

# new_directory_transformed ディレクトリ
transformed_dir = "new_directory_transformed"
transformed_files = glob.glob(os.path.join(transformed_dir, "row_*.csv"))

# シャッフル
random.seed(42)  # 再現性のためにシードを固定
random.shuffle(rinna_files)
random.shuffle(transformed_files)

# 訓練データ: 90%、テストデータ: 10%に分割
split_ratio = 0.9
split_index = int(len(rinna_files) * split_ratio)
train_rinna_files = rinna_files[:split_index]
test_rinna_files = rinna_files[split_index:]

split_index = int(len(transformed_files) * split_ratio)
train_transformed_files = transformed_files[:split_index]
test_transformed_files = transformed_files[split_index:]

print(f"Train rinna files: {len(train_rinna_files)}, Test rinna files: {len(test_rinna_files)}")
print(f"Train transformed files: {len(train_transformed_files)}, Test transformed files: {len(test_transformed_files)}")

# Datasetクラスの定義
class AccentDataset(Dataset):
    def __init__(self, rinna_files, transformed_files):
        self.data = []
        
        # rinna_F_A から特徴量を抽出
        rinna_data = []
        for rinna_file in rinna_files:
            with open(rinna_file, mode='r', newline='', encoding='utf-8') as file:
                reader = csv.DictReader(file)
                for row in reader:
                    # HuBERTで抽出された特徴量（Feature_0 〜 Feature_767）を抽出
                    features = [float(row[f"Feature_{i}"]) for i in range(768)]  # Feature_0 〜 Feature_767 をリストに格納
                    rinna_data.append(features)
        
        # new_directory_transformed からアクセントラベルを抽出
        transformed_data = []
        for rinna_file in rinna_files:
            # rinna_fileのファイル名から数字部分を取り出し、4桁の形式に変換して対応するrow_XXXX.csvを見つける
            base_name = os.path.basename(rinna_file)  # BASIC5000_0001_features.csv の形式
            file_id = base_name.split('_')[1].split('.')[0]  # 0001
            # 4桁の数字を1桁に変換し、対応するrow_1.csvに変換
            transformed_file_name = f"row_{int(file_id):d}.csv"  # row_1.csv の形式

            # 対応する transformed ファイルを検索
            transformed_file_path = os.path.join(transformed_dir, transformed_file_name)
            if os.path.exists(transformed_file_path):
                with open(transformed_file_path, mode='r', newline='', encoding='utf-8') as file:
                    reader = csv.DictReader(file)
                    for row in reader:
                        accent_label = int(row["Accent"])  # アクセント列を抽出
                        transformed_data.append(accent_label)
            else:
                print(f"Warning: Corresponding transformed file {transformed_file_name} not found.")
        
        # rinna_F_Aの特徴量とnew_directory_transformedのアクセントラベルをペアにして格納
        for features, accent_label in zip(rinna_data, transformed_data):
            self.data.append((features, accent_label))  # 特徴量とラベルをペアにして格納

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        features, label = self.data[idx]
        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)

# CSVファイルに対してデータローダーを作成
def create_dataloader(rinna_files, transformed_files):
    # Datasetの作成
    dataset = AccentDataset(rinna_files, transformed_files)
    # バッチサイズはデータの行数に設定
    batch_size = len(dataset)
    # DataLoaderの作成
    return DataLoader(dataset, batch_size=batch_size, shuffle=False)

# 訓練データとテストデータに対する処理
train_loader_list = []
test_loader_list = []

# 訓練データ用のDataLoaderを作成
for i in range(len(train_rinna_files)):
    train_loader = create_dataloader([train_rinna_files[i]], [train_transformed_files[i]])  # ファイルごとにDataLoaderを作成
    train_loader_list.append(train_loader)

# テストデータ用のDataLoaderを作成
for i in range(len(test_rinna_files)):
    test_loader = create_dataloader([test_rinna_files[i]], [test_transformed_files[i]])  # ファイルごとにDataLoaderを作成
    test_loader_list.append(test_loader)

print(f"Train loader list contains {len(train_loader_list)} data loaders.")
print(f"Test loader list contains {len(test_loader_list)} data loaders.")

# 評価データの正解ラベルを保存する
for test_file in test_rinna_files:
    all_labels = []  # このファイルの正解ラベルを保存するリスト
    
    # 評価データの元ファイル名（test_loaderに対応する評価データファイル名）
    file_name = test_file.split('/')[-1]  # ファイル名のみを取得

    # 対応するアクセントファイルを読み込み
    base_name = os.path.basename(test_file)  # BASIC5000_0001_features.csv の形式
    file_id = base_name.split('_')[1].split('.')[0]  # 0001
    # 4桁の数字を1桁に変換し、対応するrow_1.csvに変換
    corresponding_transformed_file = f"row_{int(file_id):d}.csv"  # row_1.csv の形式
    
    # transformed ファイルが存在するか確認
    transformed_file_path = os.path.join(transformed_dir, corresponding_transformed_file)
    if os.path.exists(transformed_file_path):
        with open(transformed_file_path, mode='r', newline='', encoding='utf-8') as file:
            reader = csv.DictReader(file)
            for row in reader:
                accent_label = int(row["Accent"])  # アクセント列を抽出
                all_labels.append(accent_label)
    else:
        print(f"Warning: Corresponding transformed file {corresponding_transformed_file} not found.")
    
    # 対応するrinna_F_Aファイルの行数と一致するようにアクセントラベルを保存
    with open(test_file, mode='r', newline='', encoding='utf-8') as file:
        reader = csv.reader(file)
        header = next(reader)  # ヘッダーを取得（最初の行）
        num_rows = sum(1 for row in reader) + 1  # 行数（ヘッダーも含む）
    
    # 保存するファイル名を設定
    base_file_name = os.path.splitext(file_name)[0]  # 拡張子を除いたファイル名
    output_file = os.path.join(labels_output_dir, f"{base_file_name}_labels.csv")
    
    # CSVファイルに保存
    with open(output_file, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(['True_Accent'])  # ヘッダー行の追加
        
        # 各ラベルを新しい行として書き込む
        for label in all_labels[:num_rows - 1]:  # ヘッダーを除いた行数分だけ書き込む
            writer.writerow([label])
    
    print(f"Saved true accent labels to {output_file}")

print("All labels have been saved to the specified directory.")

# モデル定義(次元数の拡張)
class TransformerModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(TransformerModel, self).__init__()
        self.linear1 = nn.Linear(input_dim, 512)  # 入力層 → 隠れ層1
        self.relu1 = nn.ReLU()
        self.linear2 = nn.Linear(512, 256)       # 隠れ層1 → 隠れ層2
        self.relu2 = nn.ReLU()
        self.linear3 = nn.Linear(256, 128)       # 隠れ層2 → 隠れ層3
        self.relu3 = nn.ReLU()
        
        # Multi-head Attention 層
        self.attention = nn.MultiheadAttention(embed_dim=128, num_heads=4, batch_first=True)
        
        # 出力層
        self.linear4 = nn.Linear(128, output_dim)  # Attention後 → 出力層

    def forward(self, x):
        x = self.linear1(x)
        x = self.relu1(x)
        x = self.linear2(x)
        x = self.relu2(x)
        x = self.linear3(x)
        x = self.relu3(x)

        # Attention層に入力するために次元を変更
        x = x.unsqueeze(1)  # [batch_size, feature_dim] -> [batch_size, seq_len=1, feature_dim]
        
        # Attentionを適用
        attn_output, _ = self.attention(x, x, x)  # クエリ, キー, バリューとしてxを使用
        
        # 次元を戻す
        attn_output = attn_output.squeeze(1)  # [batch_size, seq_len=1, feature_dim] -> [batch_size, feature_dim]

        # 出力層
        x = self.linear4(attn_output)
        return x

# ハイパーパラメータ
input_dim = 768  # 特徴量次元（Mean, Max, Min, Std, Median）
output_dim = 2   # クラス数（高: 1, 低: 0）
lr = 0.001
epochs = 100

# モデル、損失関数、オプティマイザの準備
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = TransformerModel(input_dim, output_dim).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

# 損失と精度を保存するリスト
train_losses = []
test_accuracies = []

# 訓練ループ
for epoch in range(epochs):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    # 訓練データでの損失計算
    for train_loader in train_loader_list:
        for features, labels in train_loader:
            features, labels = features.to(device), labels.to(device)

            # フォワードパス
            outputs = model(features)
            loss = criterion(outputs, labels)

            # バックプロパゲーションと最適化
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            # 精度を計算
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    # エポック毎の損失と精度を保存
    train_losses.append(total_loss / len(train_loader_list))  # 平均損失を記録
    train_accuracy = 100 * correct / total  # 訓練精度を計算

    # 訓練精度を表示
    print(f"Epoch [{epoch+1}/{epochs}], Train Loss: {total_loss / len(train_loader_list):.4f}, Train Accuracy: {train_accuracy:.2f}%")

# テストデータに対する予測(正解Ac、予測Ac)
# モデルを評価モードに変更し、予測
import os
import csv
import torch

# 保存先ディレクトリ
labels_output_dir = "predictions"
os.makedirs(labels_output_dir, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # 使用するデバイス

# モデルを評価モードに切り替え
model.eval()

# 各評価データ（test_loader_list）ごとに予測を行う
for idx, loader in enumerate(test_loader_list):
    # 対応する評価データのファイル名を取得
    test_file_name = os.path.basename(test_rinna_files[idx])  # 例: BASIC5000_0001_features.csv
    output_file_name = test_file_name.replace("features.csv", "predictions.csv")  # 例: BASIC5000_0001_predictions.csv
    output_file_path = os.path.join(labels_output_dir, output_file_name)

    all_labels = []
    all_predictions = []

    # loader内のデータを1バッチずつ処理
    for features, labels in loader:
        features, labels = features.to(device), labels.to(device)
        
        # モデルによる予測
        with torch.no_grad():  # 推論時に勾配計算を無効化
            outputs = model(features)
            _, predicted = torch.max(outputs, 1)
        
        # 実際のラベルと予測をリストに追加
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

    # 結果をファイルに保存
    with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(["True_Label", "Predicted_Label"])
        for true, pred in zip(all_labels, all_predictions):
            writer.writerow([true, pred])

    print(f"Predictions for {test_file_name} saved to {output_file_path}")

# 評価データで予測した結果が保存されたpredictionsディレクトリに、selected_filesのMora列を追加
# できてる！(保存先；combined_predictions)

import os
import csv

# 予測結果とMoraデータを保存するディレクトリ
predictions_dir = "predictions"
selected_files_dir = "selected_files"
output_dir = "combined_predictions"

# 保存先ディレクトリが存在しない場合は作成
os.makedirs(output_dir, exist_ok=True)

# predictionsディレクトリ内のファイルを順に処理
for prediction_file in os.listdir(predictions_dir):
    if prediction_file.endswith(".csv"):
        # ファイル名から番号部分（例: 'predicted_BASIC5000_0025_features.csv' -> '0025'）
        file_number = prediction_file.split('_')[1].split('.')[0]

        # 対応するmora_alignment_XXXX.csvファイルのパスを生成
        mora_file_name = f"mora_alignment_{file_number.zfill(4)}.csv"
        mora_file_path = os.path.join(selected_files_dir, mora_file_name)

        # Moraデータを読み込み
        mora_data = []
        if os.path.exists(mora_file_path):  # ファイルが存在するか確認
            with open(mora_file_path, mode='r', newline='', encoding='utf-8') as mora_file:
                reader = csv.reader(mora_file)
                next(reader)  # ヘッダー行をスキップ
                mora_data = [row[0] for row in reader]  # Mora列を抽出
        else:
            print(f"Warning: File {mora_file_name} not found in {selected_files_dir}")
            continue  # Moraファイルが見つからない場合はスキップ

        # predictionsファイルを読み込み、Moraデータを追加
        prediction_file_path = os.path.join(predictions_dir, prediction_file)
        combined_rows = []
        
        with open(prediction_file_path, mode='r', newline='', encoding='utf-8') as pred_file:
            reader = csv.reader(pred_file)
            header = next(reader)  # ヘッダー行を取得
            header.append('Mora')  # Mora列をヘッダーに追加
            combined_rows.append(header)

            # 各行にMora列を追加
            for idx, row in enumerate(reader):
                if idx < len(mora_data):
                    row.append(mora_data[idx])  # Mora列を追加
                    combined_rows.append(row)

        # 新しいファイルを保存
        output_file_path = os.path.join(output_dir, prediction_file)
        with open(output_file_path, mode='w', newline='', encoding='utf-8') as output_file:
            writer = csv.writer(output_file)
            writer.writerows(combined_rows)

        print(f"File {prediction_file} processed and saved to {output_file_path}")

# アクセント上昇、下降位置における
# 一致率、再現率、適合率、F値
import os
import csv
from sklearn.metrics import recall_score, precision_score, f1_score

# combined_predictions ディレクトリ
combined_predictions_dir = "combined_predictions"
output_results_dir = "results"
os.makedirs(output_results_dir, exist_ok=True)

# 集計用の変数
total_up = total_down = correct_up = correct_down = 0
true_labels_all = []
predicted_labels_all = []

# combined_predictions ディレクトリ内のファイルを順に処理
for prediction_file in os.listdir(combined_predictions_dir):
    if prediction_file.endswith(".csv"):
        prediction_file_path = os.path.join(combined_predictions_dir, prediction_file)

        true_labels = []
        predicted_labels = []

        # True Label, Predicted Label 列を読み込み
        with open(prediction_file_path, mode='r', newline='', encoding='utf-8') as file:
            reader = csv.DictReader(file)
            
            # ヘッダー情報を表示
            # print(f"Headers in {prediction_file}: {reader.fieldnames}")   # デバッグ用
            
            for row in reader:
                # ヘッダーに応じて列名を指定（ここを調整する必要がある場合あり）
                true_labels.append(int(row["True_Label"]))  # "True Label" に変更
                predicted_labels.append(int(row["Predicted_Label"]))  # "Predicted Label" に変更

        # アクセント変化を検出
        true_changes = []
        predicted_changes = []

        for i in range(1, len(true_labels)):
            # True Label の変化 (アクセント上昇、下降)
            if true_labels[i-1] == 0 and true_labels[i] == 1:
                true_changes.append('up')
            elif true_labels[i-1] == 1 and true_labels[i] == 0:
                true_changes.append('down')
            else:
                true_changes.append('no_change')

            # Predicted Label の変化 (アクセント上昇、下降)
            if predicted_labels[i-1] == 0 and predicted_labels[i] == 1:
                predicted_changes.append('up')
            elif predicted_labels[i-1] == 1 and predicted_labels[i] == 0:
                predicted_changes.append('down')
            else:
                predicted_changes.append('no_change')

        # アクセント上昇、下降の一致率を計算
        for true, pred in zip(true_changes, predicted_changes):
            if true == 'up':
                total_up += 1
                if pred == 'up':
                    correct_up += 1
            elif true == 'down':
                total_down += 1
                if pred == 'down':
                    correct_down += 1

        # 全体の真のラベルと予測ラベルを収集
        true_labels_all.extend(true_labels)
        predicted_labels_all.extend(predicted_labels)

# 一致率を計算
accuracy_up = correct_up / total_up if total_up > 0 else 0
accuracy_down = correct_down / total_down if total_down > 0 else 0

# 再現率、適合率、F1スコアを計算
recall_up = recall_score(true_labels_all, predicted_labels_all, pos_label=1)
precision_up = precision_score(true_labels_all, predicted_labels_all, pos_label=1)
f1_up = f1_score(true_labels_all, predicted_labels_all, pos_label=1)

recall_down = recall_score(true_labels_all, predicted_labels_all, pos_label=0)
precision_down = precision_score(true_labels_all, predicted_labels_all, pos_label=0)
f1_down = f1_score(true_labels_all, predicted_labels_all, pos_label=0)

# 結果の表示
print(f"Total Up Changes: {total_up}, Correct Up Predictions: {correct_up}, Accuracy (Up): {accuracy_up:.4f}")
print(f"Total Down Changes: {total_down}, Correct Down Predictions: {correct_down}, Accuracy (Down): {accuracy_down:.4f}")

print(f"Recall (Up): {recall_up:.4f}")
print(f"Precision (Up): {precision_up:.4f}")
print(f"F1 Score (Up): {f1_up:.4f}")

print(f"Recall (Down): {recall_down:.4f}")
print(f"Precision (Down): {precision_down:.4f}")
print(f"F1 Score (Down): {f1_down:.4f}")

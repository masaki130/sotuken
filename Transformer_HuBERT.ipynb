{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files: 3980\n",
      "Evaluation files: 996\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# ディレクトリ内のCSVファイルを取得\n",
    "data_dir = \"basic5000_features_with_accent\"\n",
    "csv_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "# 学習データと評価データに分割（80%学習、20%評価）\n",
    "random.shuffle(csv_files)\n",
    "train_files = csv_files[:int(0.8 * len(csv_files))]\n",
    "eval_files = csv_files[int(0.8 * len(csv_files)):]\n",
    "\n",
    "print(f\"Training files: {len(train_files)}\")\n",
    "print(f\"Evaluation files: {len(eval_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def process_csv(file_path):\n",
    "    \"\"\"\n",
    "    CSVファイルを読み込み、音素、特徴量、アクセントに分割\n",
    "    \"\"\"\n",
    "    moras = []  # 音素リスト\n",
    "    features = []  # 特徴量リスト\n",
    "    accents = []  # アクセントリスト\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # ヘッダー行をスキップ\n",
    "        for row in reader:\n",
    "            # 音素は1列目\n",
    "            mora = row[0]\n",
    "            # 特徴量は2列目から最後の1つ手前まで\n",
    "            feature = list(map(float, row[1:-1]))\n",
    "            # アクセントは最後の列\n",
    "            accent = int(row[-1])\n",
    "\n",
    "            # データを格納\n",
    "            moras.append(mora)\n",
    "            features.append(feature)\n",
    "            accents.append(accent)\n",
    "    \n",
    "    return moras, features, accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 141414 samples\n",
      "Evaluation data: 35696 samples\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_list):\n",
    "    \"\"\"\n",
    "    ファイルリストから特徴量とアクセントをまとめてロード\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    all_accents = []\n",
    "\n",
    "    for file_path in file_list:\n",
    "        _, features, accents = process_csv(file_path)\n",
    "        all_features.extend(features)\n",
    "        all_accents.extend(accents)\n",
    "    \n",
    "    return all_features, all_accents\n",
    "\n",
    "# データロード\n",
    "train_features, train_accents = load_data(train_files)\n",
    "eval_features, eval_accents = load_data(eval_files)\n",
    "\n",
    "print(f\"Training data: {len(train_features)} samples\")\n",
    "print(f\"Evaluation data: {len(eval_features)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, features, accents):\n",
    "        self.features = features\n",
    "        self.accents = accents\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.accents[idx], dtype=torch.long)\n",
    "\n",
    "# データセット作成\n",
    "train_dataset = AccentDataset(train_features, train_accents)\n",
    "eval_dataset = AccentDataset(eval_features, eval_accents)\n",
    "\n",
    "# データローダー作成\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AccentTransformer(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes=2, num_heads=4, num_layers=2, hidden_dim=128):\n",
    "        super(AccentTransformer, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=feature_dim, nhead=num_heads, dim_feedforward=hidden_dim),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(feature_dim, num_classes)  # 出力層\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Transformerにデータを通す\n",
    "        x = self.encoder(x.unsqueeze(1))  # バッチサイズ x 1 x 特徴量次元\n",
    "        x = x.squeeze(1)  # バッチサイズ x 特徴量次元\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/4420 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 4420/4420 [01:52<00:00, 39.44batch/s, loss=0.617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7007573174405421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 4420/4420 [01:49<00:00, 40.34batch/s, loss=0.688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.6944000630626851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 4420/4420 [01:39<00:00, 44.63batch/s, loss=0.681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.6936208206334266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 4420/4420 [01:29<00:00, 49.14batch/s, loss=0.714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.6935250591233845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 4420/4420 [01:28<00:00, 49.67batch/s, loss=0.691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.6932381208126361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 4420/4420 [01:26<00:00, 51.30batch/s, loss=0.667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.6931737520171506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 4420/4420 [01:25<00:00, 51.50batch/s, loss=0.698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.693161761086451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 4420/4420 [01:24<00:00, 52.54batch/s, loss=0.697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.6931368832135093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 4420/4420 [01:25<00:00, 51.93batch/s, loss=0.675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.693089071873626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 4420/4420 [01:23<00:00, 52.97batch/s, loss=0.704]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.6930611639136103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# モデル定義\n",
    "feature_dim = len(train_features[0])  # 特徴量の次元数\n",
    "model = AccentTransformer(feature_dim)\n",
    "\n",
    "# 最適化器と損失関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 学習ループ\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # tqdm を使用して進捗バーを表示\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for features, labels in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # tqdm の表示を更新\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files: 3980\n",
      "Evaluation files: 996\n"
     ]
    }
   ],
   "source": [
    "# ここから！！\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ディレクトリ内のCSVファイルを取得\n",
    "data_dir = \"basic5000_features_with_accent\"\n",
    "csv_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "# 学習データと評価データに分割（80%学習、20%評価）\n",
    "random.shuffle(csv_files)\n",
    "train_files = csv_files[:int(0.8 * len(csv_files))]\n",
    "eval_files = csv_files[int(0.8 * len(csv_files)):]\n",
    "\n",
    "print(f\"Training files: {len(train_files)}\")\n",
    "print(f\"Evaluation files: {len(eval_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 3980 samples\n",
      "Evaluation data: 996 samples\n"
     ]
    }
   ],
   "source": [
    "def process_csv(file_path):\n",
    "    \"\"\"\n",
    "    CSVファイルを読み込み、音素、特徴量、アクセントに分割\n",
    "    \"\"\"\n",
    "    moras = []  # 音素リスト\n",
    "    features = []  # 特徴量リスト\n",
    "    accents = []  # アクセントリスト\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # ヘッダー行をスキップ\n",
    "        for row in reader:\n",
    "            # 音素は1列目\n",
    "            mora = row[0]\n",
    "            # 特徴量は2列目から最後の1つ手前まで\n",
    "            feature = list(map(float, row[1:-1]))\n",
    "            # アクセントは最後の列\n",
    "            accent = int(row[-1])\n",
    "\n",
    "            # データを格納\n",
    "            moras.append(mora)\n",
    "            features.append(feature)\n",
    "            accents.append(accent)\n",
    "    \n",
    "    return moras, features, accents\n",
    "\n",
    "def load_data(file_list):\n",
    "    \"\"\"\n",
    "    ファイルリストから特徴量とアクセントをまとめてロード\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    all_accents = []\n",
    "\n",
    "    for file_path in file_list:\n",
    "        _, features, accents = process_csv(file_path)\n",
    "        all_features.append(features)\n",
    "        all_accents.append(accents)\n",
    "    \n",
    "    return all_features, all_accents\n",
    "\n",
    "# データロード\n",
    "train_features, train_accents = load_data(train_files)\n",
    "eval_features, eval_accents = load_data(eval_files)\n",
    "\n",
    "print(f\"Training data: {len(train_features)} samples\")\n",
    "print(f\"Evaluation data: {len(eval_features)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# カスタムcollate_fn\n",
    "def collate_fn(batch):\n",
    "    features, accents = zip(*batch)\n",
    "    \n",
    "    # pad_sequenceでパディング\n",
    "    features_padded = pad_sequence(features, batch_first=True, padding_value=0.0)  # バッチサイズ x 最大シーケンス長 x 特徴量次元\n",
    "    \n",
    "    # アクセントラベル\n",
    "    accents = torch.stack(accents, dim=0)\n",
    "    \n",
    "    return features_padded, accents\n",
    "\n",
    "# AccentDatasetクラス\n",
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, features, accents):\n",
    "        self.features = features\n",
    "        self.accents = accents\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        accent = torch.tensor(self.accents[idx], dtype=torch.long)\n",
    "        return feature, accent\n",
    "\n",
    "# データローダー作成（collate_fnを使用）\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class AccentTransformer(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes=2, num_heads=4, num_layers=2, hidden_dim=128):\n",
    "        super(AccentTransformer, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=feature_dim, nhead=num_heads, dim_feedforward=hidden_dim),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(feature_dim, num_classes)  # 出力層\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: バッチサイズ x 最大シーケンス長 x 特徴量次元\n",
    "        src_key_padding_mask = (x == 0).all(dim=-1)  # パディング部分をTrueにする\n",
    "        x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        x = x.mean(dim=1)  # シーケンス全体の平均を取る\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "# モデル定義\n",
    "feature_dim = len(train_features[0][0])  # 特徴量の次元数\n",
    "model = AccentTransformer(feature_dim)\n",
    "\n",
    "# 最適化器と損失関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/125 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [32] at entry 0 and [40] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# tqdm を使用して進捗バーを表示\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m features, labels \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     10\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(features)\n",
      "File \u001b[0;32m~/.conda/envs/sotuken/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sotuken/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/sotuken/lib/python3.9/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/sotuken/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 12\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      9\u001b[0m features_padded \u001b[38;5;241m=\u001b[39m pad_sequence(features, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)  \u001b[38;5;66;03m# バッチサイズ x 最大シーケンス長 x 特徴量次元\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# アクセントラベル\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m accents \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features_padded, accents\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [32] at entry 0 and [40] at entry 1"
     ]
    }
   ],
   "source": [
    "# 学習ループ\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # tqdm を使用して進捗バーを表示\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for features, labels in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # tqdm の表示を更新\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # 評価データでの評価\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}, Evaluation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# モデルの保存\n",
    "torch.save(model.state_dict(), \"accent_transformer_model.pth\")\n",
    "print(\"Model saved as 'accent_transformer_model.pth'\")\n",
    "\n",
    "# 学習が完了した後に、最終的な評価結果を出力\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for features, labels in eval_loader:\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "final_accuracy = 100 * correct / total\n",
    "print(f\"Final Evaluation Accuracy: {final_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データの次元数が揃わないと学習できないっぽい！\n",
    "# 次元を揃える必要がある\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sotuken",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

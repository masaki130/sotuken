{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def load_and_preprocess_data(folder_path):\n",
    "    csv_files = glob.glob(f\"{folder_path}/*.csv\")\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        data = df[['Mean', 'Max', 'Min', 'Std', 'Median', 'Accent']].values\n",
    "        data_list.append(data)\n",
    "    \n",
    "    return np.concatenate(data_list, axis=0)\n",
    "\n",
    "# データのロード\n",
    "data = load_and_preprocess_data('f0_stac_result_5000')\n",
    "\n",
    "# 特徴量とラベルを分ける\n",
    "X = data[:, :-1]  # 特徴量（Mean, Max, Min, Std, Median）\n",
    "y = data[:, -1]   # ラベル（Accent）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここから！\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reshape_data関数を定義\n",
    "def reshape_data(X, time_steps=1):\n",
    "    X_reshaped = []\n",
    "    for i in range(len(X) - time_steps + 1):\n",
    "        X_reshaped.append(X[i:i + time_steps])\n",
    "    return np.array(X_reshaped)\n",
    "\n",
    "# データの整形\n",
    "time_steps = 5\n",
    "X_reshaped = reshape_data(X, time_steps=time_steps)\n",
    "y_reshaped = y[time_steps - 1:]\n",
    "\n",
    "# データの分割（80%を学習、20%を評価）\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_reshaped, y_reshaped, test_size=0.2, random_state=42)\n",
    "\n",
    "# データの形状確認\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "\n",
    "# データをPyTorchのテンソルに変換\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# データセットとデータローダーの作成\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        out = self.fc(hn[-1])\n",
    "        return out\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "input_size = X_train_tensor.shape[2]\n",
    "hidden_size = 50\n",
    "output_size = 3  # アクセントのクラス数（0, 1, 2）\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# 損失関数とオプティマイザの定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 評価結果を保存するリスト\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 学習\n",
    "num_epochs = 50\n",
    "epoch_losses = []  # 各エポックの平均損失を格納するリスト\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルをトレーニングモードに設定\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "    \n",
    "    average_loss = epoch_loss / len(train_dataloader)\n",
    "    epoch_losses.append(average_loss)  # 平均損失をリストに追加\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# 損失の折れ線グラフを描画\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o', linestyle='-', color='b')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.xticks(range(1, num_epochs + 1))  # x軸の目盛りを設定\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エポック数：100\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reshape_data関数を定義\n",
    "def reshape_data(X, time_steps=1):\n",
    "    X_reshaped = []\n",
    "    for i in range(len(X) - time_steps + 1):\n",
    "        X_reshaped.append(X[i:i + time_steps])\n",
    "    return np.array(X_reshaped)\n",
    "\n",
    "# データの整形\n",
    "time_steps = 5\n",
    "X_reshaped = reshape_data(X, time_steps=time_steps)\n",
    "y_reshaped = y[time_steps - 1:]\n",
    "\n",
    "# データの分割（80%を学習、20%を評価）\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_reshaped, y_reshaped, test_size=0.2, random_state=42)\n",
    "\n",
    "# データの形状確認\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "\n",
    "# データをPyTorchのテンソルに変換\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# データセットとデータローダーの作成\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        out = self.fc(hn[-1])\n",
    "        return out\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "input_size = X_train_tensor.shape[2]\n",
    "hidden_size = 50\n",
    "output_size = 3  # アクセントのクラス数（0, 1, 2）\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# 損失関数とオプティマイザの定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 評価結果を保存するリスト\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 学習\n",
    "num_epochs = 100\n",
    "epoch_losses = []  # 各エポックの平均損失を格納するリスト\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルをトレーニングモードに設定\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "    \n",
    "    average_loss = epoch_loss / len(train_dataloader)\n",
    "    epoch_losses.append(average_loss)  # 平均損失をリストに追加\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# 損失の折れ線グラフを描画\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o', linestyle='-', color='b')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.xticks(range(1, num_epochs + 1))  # x軸の目盛りを設定\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここから\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def load_and_preprocess_data(folder_path):\n",
    "    csv_files = glob.glob(f\"{folder_path}/*.csv\")\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        data = df[['Mean', 'Max', 'Min', 'Std', 'Median', 'Accent']].values\n",
    "        data_list.append(data)\n",
    "    \n",
    "    return np.concatenate(data_list, axis=0)\n",
    "\n",
    "# データのロード\n",
    "data = load_and_preprocess_data('f0_stac_result_5000')\n",
    "\n",
    "# 特徴量とラベルを分ける\n",
    "X = data[:, :-1]  # 特徴量（Mean, Max, Min, Std, Median）\n",
    "y = data[:, -1]   # ラベル（Accent）\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# reshape_data関数を定義\n",
    "def reshape_data(X, time_steps=1):\n",
    "    X_reshaped = []\n",
    "    \n",
    "    for i in range(len(X) - time_steps + 1):  # 修正: +1を追加\n",
    "        X_reshaped.append(X[i:i + time_steps])\n",
    "        \n",
    "    return np.array(X_reshaped)\n",
    "\n",
    "# データの整形\n",
    "time_steps = 5  # 5つの時間ステップを使用\n",
    "X_reshaped = reshape_data(X, time_steps=time_steps)\n",
    "y_reshaped = y[time_steps - 1:]  # 最初の4つはシーケンスが足りないためカット\n",
    "\n",
    "# データの形状確認\n",
    "print(X_reshaped.shape, y_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エポック数；120\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reshape_data関数を定義\n",
    "def reshape_data(X, time_steps=1):\n",
    "    X_reshaped = []\n",
    "    for i in range(len(X) - time_steps + 1):\n",
    "        X_reshaped.append(X[i:i + time_steps])\n",
    "    return np.array(X_reshaped)\n",
    "\n",
    "# データの整形\n",
    "time_steps = 5\n",
    "X_reshaped = reshape_data(X, time_steps=time_steps)\n",
    "y_reshaped = y[time_steps - 1:]\n",
    "\n",
    "# データの分割（80%を学習、20%を評価）\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_reshaped, y_reshaped, test_size=0.2, random_state=42)\n",
    "\n",
    "# データの形状確認\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "\n",
    "# データをPyTorchのテンソルに変換\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# データセットとデータローダーの作成\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        out = self.fc(hn[-1])\n",
    "        return out\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "input_size = X_train_tensor.shape[2]\n",
    "hidden_size = 50\n",
    "output_size = 3  # アクセントのクラス数（0, 1, 2）\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# 損失関数とオプティマイザの定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 評価結果を保存するリスト\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 学習\n",
    "num_epochs = 120\n",
    "epoch_losses = []  # 各エポックの平均損失を格納するリスト\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルをトレーニングモードに設定\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "    \n",
    "    average_loss = epoch_loss / len(train_dataloader)\n",
    "    epoch_losses.append(average_loss)  # 平均損失をリストに追加\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# 損失の折れ線グラフを描画\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o', linestyle='-', color='b')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.xticks(range(1, num_epochs + 1))  # x軸の目盛りを設定\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def load_and_preprocess_data(folder_path):\n",
    "    csv_files = glob.glob(f\"{folder_path}/*.csv\")\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        data = df[['Mean', 'Max', 'Min', 'Std', 'Median', 'Accent']].values\n",
    "        data_list.append(data)\n",
    "    \n",
    "    return np.concatenate(data_list, axis=0)\n",
    "\n",
    "# データのロード\n",
    "data = load_and_preprocess_data('f0_stac_result_5000')\n",
    "\n",
    "# 特徴量とラベルを分ける\n",
    "X = data[:, :-1]  # 特徴量（Mean, Max, Min, Std, Median）\n",
    "y = data[:, -1]   # ラベル（Accent）\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# reshape_data関数を定義\n",
    "def reshape_data(X, time_steps=1):\n",
    "    X_reshaped = []\n",
    "    \n",
    "    for i in range(len(X) - time_steps + 1):  # 修正: +1を追加\n",
    "        X_reshaped.append(X[i:i + time_steps])\n",
    "        \n",
    "    return np.array(X_reshaped)\n",
    "\n",
    "# データの整形\n",
    "time_steps = 5  # 5つの時間ステップを使用\n",
    "X_reshaped = reshape_data(X, time_steps=time_steps)\n",
    "y_reshaped = y[time_steps - 1:]  # 最初の4つはシーケンスが足りないためカット\n",
    "\n",
    "# データの形状確認\n",
    "print(X_reshaped.shape, y_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reshape_data関数を定義\n",
    "def reshape_data(X, time_steps=1):\n",
    "    X_reshaped = []\n",
    "    for i in range(len(X) - time_steps + 1):\n",
    "        X_reshaped.append(X[i:i + time_steps])\n",
    "    return np.array(X_reshaped)\n",
    "\n",
    "# データの整形\n",
    "time_steps = 5\n",
    "X_reshaped = reshape_data(X, time_steps=time_steps)\n",
    "y_reshaped = y[time_steps - 1:]\n",
    "\n",
    "# データの分割（80%を学習、20%を評価）\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_reshaped, y_reshaped, test_size=0.2, random_state=42)\n",
    "\n",
    "# データの形状確認\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "\n",
    "# データをPyTorchのテンソルに変換\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# データセットとデータローダーの作成\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        out = self.fc(hn[-1])\n",
    "        return out\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "input_size = X_train_tensor.shape[2]\n",
    "hidden_size = 100\n",
    "output_size = 3  # アクセントのクラス数（0, 1, 2）\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# 損失関数とオプティマイザの定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 評価結果を保存するリスト\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 学習\n",
    "num_epochs = 100\n",
    "epoch_losses = []  # 各エポックの平均損失を格納するリスト\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルをトレーニングモードに設定\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "    \n",
    "    average_loss = epoch_loss / len(train_dataloader)\n",
    "    epoch_losses.append(average_loss)  # 平均損失をリストに追加\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# 損失の折れ線グラフを描画\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o', linestyle='-', color='b')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.xticks(range(1, num_epochs + 1))  # x軸の目盛りを設定\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 双方向LSTM+２つのドロップアウト層\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def load_and_preprocess_data(folder_path):\n",
    "    csv_files = glob.glob(f\"{folder_path}/*.csv\")\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        data = df[['Mean', 'Max', 'Min', 'Std', 'Median', 'Accent']].values\n",
    "        data_list.append(data)\n",
    "    \n",
    "    return np.concatenate(data_list, axis=0)\n",
    "\n",
    "# データのロード\n",
    "data = load_and_preprocess_data('f0_stac_result_5000')\n",
    "\n",
    "# 特徴量とラベルを分ける\n",
    "X = data[:, :-1]  # 特徴量（Mean, Max, Min, Std, Median）\n",
    "y = data[:, -1]   # ラベル（Accent）\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# reshape_data関数を定義\n",
    "def reshape_data(X, time_steps=1):\n",
    "    X_reshaped = []\n",
    "    \n",
    "    for i in range(len(X) - time_steps + 1):  # 修正: +1を追加\n",
    "        X_reshaped.append(X[i:i + time_steps])\n",
    "        \n",
    "    return np.array(X_reshaped)\n",
    "\n",
    "# データの整形\n",
    "time_steps = 5  # 5つの時間ステップを使用\n",
    "X_reshaped = reshape_data(X, time_steps=time_steps)\n",
    "y_reshaped = y[time_steps - 1:]  # 最初の4つはシーケンスが足りないためカット\n",
    "\n",
    "# データの形状確認\n",
    "print(X_reshaped.shape, y_reshaped.shape)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reshape_data関数を定義\n",
    "def reshape_data(X, time_steps=1):\n",
    "    X_reshaped = []\n",
    "    for i in range(len(X) - time_steps + 1):\n",
    "        X_reshaped.append(X[i:i + time_steps])\n",
    "    return np.array(X_reshaped)\n",
    "\n",
    "# データの整形\n",
    "time_steps = 5\n",
    "X_reshaped = reshape_data(X, time_steps=time_steps)\n",
    "y_reshaped = y[time_steps - 1:]\n",
    "\n",
    "# データの分割（80%を学習、20%を評価）\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_reshaped, y_reshaped, test_size=0.2, random_state=42)\n",
    "\n",
    "# データの形状確認\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "\n",
    "# データをPyTorchのテンソルに変換\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# データセットとデータローダーの作成\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# BiLSTMモデルの定義\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout_prob=0.5):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        # 2つの双方向LSTM層を設定\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, \n",
    "                            dropout=dropout_prob, bidirectional=True, batch_first=True)\n",
    "        # ドロップアウト層を設定\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # 双方向LSTMなのでhidden_size * 2を次元に持つ全結合層\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM層を通過\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        # ドロップアウトを適用\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        # 双方向なので、順方向と逆方向の出力を結合 (hidden statesの最後を使用)\n",
    "        hn_concat = torch.cat((hn[-2], hn[-1]), dim=1)  # 双方向の最終層の出力を結合\n",
    "        # 全結合層に通して出力\n",
    "        out = self.fc(hn_concat)\n",
    "        return out\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "input_size = X_train_tensor.shape[2]\n",
    "hidden_size = 50\n",
    "output_size = 3  # アクセントのクラス数（0, 1, 2）\n",
    "num_layers = 2\n",
    "dropout_prob = 0.5\n",
    "\n",
    "model = BiLSTMModel(input_size, hidden_size, output_size, num_layers=num_layers, dropout_prob=dropout_prob)\n",
    "\n",
    "# 損失関数とオプティマイザの定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 評価結果を保存するリスト\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 学習\n",
    "num_epochs = 100\n",
    "epoch_losses = []  # 各エポックの平均損失を格納するリスト\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルをトレーニングモードに設定\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "    \n",
    "    average_loss = epoch_loss / len(train_dataloader)\n",
    "    epoch_losses.append(average_loss)  # 平均損失をリストに追加\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# 損失の折れ線グラフを描画\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o', linestyle='-', color='b')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.xticks(range(1, num_epochs + 1))  # x軸の目盛りを設定\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 双方向LSTM+２つのドロップアウト層\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def load_and_preprocess_data(folder_path):\n",
    "    csv_files = glob.glob(f\"{folder_path}/*.csv\")\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        data = df[['Mean', 'Max', 'Min', 'Std', 'Median', 'Accent']].values\n",
    "        data_list.append(data)\n",
    "    \n",
    "    return np.concatenate(data_list, axis=0)\n",
    "\n",
    "# データのロード\n",
    "data = load_and_preprocess_data('f0_stac_result_5000')\n",
    "\n",
    "# 特徴量とラベルを分ける\n",
    "X = data[:, :-1]  # 特徴量（Mean, Max, Min, Std, Median）\n",
    "y = data[:, -1]   # ラベル（Accent）\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# reshape_data関数を定義\n",
    "def reshape_data(X, time_steps=1):\n",
    "    X_reshaped = []\n",
    "    \n",
    "    for i in range(len(X) - time_steps + 1):  # 修正: +1を追加\n",
    "        X_reshaped.append(X[i:i + time_steps])\n",
    "        \n",
    "    return np.array(X_reshaped)\n",
    "\n",
    "# データの整形\n",
    "time_steps = 5  # 5つの時間ステップを使用\n",
    "X_reshaped = reshape_data(X, time_steps=time_steps)\n",
    "y_reshaped = y[time_steps - 1:]  # 最初の4つはシーケンスが足りないためカット\n",
    "\n",
    "# データの形状確認\n",
    "print(X_reshaped.shape, y_reshaped.shape)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reshape_data関数を定義\n",
    "def reshape_data(X, time_steps=1):\n",
    "    X_reshaped = []\n",
    "    for i in range(len(X) - time_steps + 1):\n",
    "        X_reshaped.append(X[i:i + time_steps])\n",
    "    return np.array(X_reshaped)\n",
    "\n",
    "# データの整形\n",
    "time_steps = 5\n",
    "X_reshaped = reshape_data(X, time_steps=time_steps)\n",
    "y_reshaped = y[time_steps - 1:]\n",
    "\n",
    "# データの分割（80%を学習、20%を評価）\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_reshaped, y_reshaped, test_size=0.2, random_state=42)\n",
    "\n",
    "# データの形状確認\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "\n",
    "# データをPyTorchのテンソルに変換\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# データセットとデータローダーの作成\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# BiLSTMモデルの定義\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout_prob=0.5):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        # 2つの双方向LSTM層を設定\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, \n",
    "                            dropout=dropout_prob, bidirectional=True, batch_first=True)\n",
    "        # ドロップアウト層を設定\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # 双方向LSTMなのでhidden_size * 2を次元に持つ全結合層\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM層を通過\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        # ドロップアウトを適用\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        # 双方向なので、順方向と逆方向の出力を結合 (hidden statesの最後を使用)\n",
    "        hn_concat = torch.cat((hn[-2], hn[-1]), dim=1)  # 双方向の最終層の出力を結合\n",
    "        # 全結合層に通して出力\n",
    "        out = self.fc(hn_concat)\n",
    "        return out\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "input_size = X_train_tensor.shape[2]\n",
    "hidden_size = 50\n",
    "output_size = 3  # アクセントのクラス数（0, 1, 2）\n",
    "num_layers = 2\n",
    "dropout_prob = 0.5\n",
    "\n",
    "model = BiLSTMModel(input_size, hidden_size, output_size, num_layers=num_layers, dropout_prob=dropout_prob)\n",
    "\n",
    "# 損失関数とオプティマイザの定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 評価結果を保存するリスト\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 学習\n",
    "num_epochs = 120\n",
    "epoch_losses = []  # 各エポックの平均損失を格納するリスト\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルをトレーニングモードに設定\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "    \n",
    "    average_loss = epoch_loss / len(train_dataloader)\n",
    "    epoch_losses.append(average_loss)  # 平均損失をリストに追加\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# 損失の折れ線グラフを描画\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o', linestyle='-', color='b')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.xticks(range(1, num_epochs + 1))  # x軸の目盛りを設定\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 双方向LSTM+２つのドロップアウト層\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def load_and_preprocess_data(folder_path):\n",
    "    csv_files = glob.glob(f\"{folder_path}/*.csv\")\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        data = df[['Mean', 'Max', 'Min', 'Std', 'Median', 'Accent']].values\n",
    "        data_list.append(data)\n",
    "    \n",
    "    return np.concatenate(data_list, axis=0)\n",
    "\n",
    "# データのロード\n",
    "data = load_and_preprocess_data('f0_stac_result_5000')  # 特徴量とラベルが一緒のデータ\n",
    "\n",
    "# 特徴量とラベルを分ける\n",
    "X = data[:, :-1]  # 特徴量（Mean, Max, Min, Std, Median）\n",
    "y = data[:, -1]   # ラベル（Accent）\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# reshape_data関数を定義\n",
    "def reshape_data(X, time_steps=1):\n",
    "    X_reshaped = []\n",
    "    \n",
    "    for i in range(len(X) - time_steps + 1):  # 修正: +1を追加\n",
    "        X_reshaped.append(X[i:i + time_steps])\n",
    "        \n",
    "    return np.array(X_reshaped)\n",
    "\n",
    "# データの整形\n",
    "time_steps = 5  # 5つの時間ステップを使用\n",
    "X_reshaped = reshape_data(X, time_steps=time_steps)\n",
    "y_reshaped = y[time_steps - 1:]  # 最初の4つはシーケンスが足りないためカット\n",
    "\n",
    "# データの形状確認\n",
    "print(X_reshaped.shape, y_reshaped.shape)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reshape_data関数を定義\n",
    "def reshape_data(X, time_steps=1):\n",
    "    X_reshaped = []\n",
    "    for i in range(len(X) - time_steps + 1):\n",
    "        X_reshaped.append(X[i:i + time_steps])\n",
    "    return np.array(X_reshaped)\n",
    "\n",
    "# データの整形\n",
    "time_steps = 5\n",
    "X_reshaped = reshape_data(X, time_steps=time_steps)\n",
    "y_reshaped = y[time_steps - 1:]\n",
    "\n",
    "# データの分割（80%を学習、20%を評価）\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_reshaped, y_reshaped, test_size=0.2, random_state=42)\n",
    "\n",
    "# データの形状確認\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "\n",
    "# データをPyTorchのテンソルに変換\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# データセットとデータローダーの作成\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# BiLSTMモデルの定義\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout_prob=0.5):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        # 2つの双方向LSTM層を設定\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, \n",
    "                            dropout=dropout_prob, bidirectional=True, batch_first=True)\n",
    "        # ドロップアウト層を設定\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # 双方向LSTMなのでhidden_size * 2を次元に持つ全結合層\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM層を通過\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        # ドロップアウトを適用\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        # 双方向なので、順方向と逆方向の出力を結合 (hidden statesの最後を使用)\n",
    "        hn_concat = torch.cat((hn[-2], hn[-1]), dim=1)  # 双方向の最終層の出力を結合\n",
    "        # 全結合層に通して出力\n",
    "        out = self.fc(hn_concat)\n",
    "        return out\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "input_size = X_train_tensor.shape[2]\n",
    "hidden_size = 50\n",
    "output_size = 3  # アクセントのクラス数（0, 1, 2）\n",
    "num_layers = 2\n",
    "dropout_prob = 0.5\n",
    "\n",
    "model = BiLSTMModel(input_size, hidden_size, output_size, num_layers=num_layers, dropout_prob=dropout_prob)\n",
    "\n",
    "# 損失関数とオプティマイザの定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 評価結果を保存するリスト\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 学習\n",
    "num_epochs = 150\n",
    "epoch_losses = []  # 各エポックの平均損失を格納するリスト\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルをトレーニングモードに設定\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            pbar.update(1)\n",
    "    \n",
    "    average_loss = epoch_loss / len(train_dataloader)\n",
    "    epoch_losses.append(average_loss)  # 平均損失をリストに追加\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_loss:.4f}')\n",
    "\n",
    "# 損失の折れ線グラフを描画\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o', linestyle='-', color='b')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.xticks(range(1, num_epochs + 1))  # x軸の目盛りを設定\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def load_and_preprocess_data(folder_path):\n",
    "    csv_files = glob.glob(f\"{folder_path}/*.csv\")\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        data = df[['Mean', 'Max', 'Min', 'Std', 'Median', 'Accent']].values\n",
    "        data_list.append(data)\n",
    "    \n",
    "    return np.concatenate(data_list, axis=0)\n",
    "\n",
    "# データのロード\n",
    "data = load_and_preprocess_data('f0_stac_result_5000')\n",
    "\n",
    "# 特徴量とラベルを分ける\n",
    "X = data[:, :-1]  # 特徴量（Mean, Max, Min, Std, Median）\n",
    "y = data[:, -1]   # ラベル（Accent）\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def load_and_preprocess_data(folder_path):\n",
    "    csv_files = glob.glob(f\"{folder_path}/*.csv\")\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        data = df[['Mean', 'Max', 'Min', 'Std', 'Median', 'Accent']].values\n",
    "        data_list.append(data)\n",
    "    \n",
    "    return np.concatenate(data_list, axis=0)\n",
    "\n",
    "# データのロード\n",
    "data = load_and_preprocess_data('f0_stac_result_5000')\n",
    "\n",
    "# 特徴量とラベルを分ける\n",
    "X = data[:, :-1]  # 特徴量（Mean, Max, Min, Std, Median）\n",
    "y = data[:, -1]   # ラベル（Accent）\n",
    "\n",
    "# 特徴量をDataFrameに変換してCSVファイルとして保存\n",
    "X_df = pd.DataFrame(X, columns=['Mean', 'Max', 'Min', 'Std', 'Median'])\n",
    "X_df.to_csv('output_features.csv', index=False)  # index=Falseでインデックスを保存しない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各音声の区切りがわからない\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def load_and_preprocess_data(folder_path):\n",
    "    # f0_statistics_1.csv から f0_statistics_5000.csv までのファイルを指定して読み込む\n",
    "    csv_files = [f\"{folder_path}/f0_statistics_{i}.csv\" for i in range(1, 5001)]\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        # 必要な列を取得\n",
    "        data = df[['Mean', 'Max', 'Min', 'Std', 'Median', 'Accent']].values\n",
    "        data_list.append(data)\n",
    "    \n",
    "    return np.concatenate(data_list, axis=0)\n",
    "\n",
    "# データのロード\n",
    "data = load_and_preprocess_data('f0_stac_result_5000')\n",
    "\n",
    "# 特徴量とラベルを分ける\n",
    "X = data[:, :-1]  # 特徴量（Mean, Max, Min, Std, Median）\n",
    "y = data[:, -1]   # ラベル（Accent）\n",
    "\n",
    "# 特徴量をDataFrameに変換してCSVファイルとして保存\n",
    "X_df = pd.DataFrame(X, columns=['Mean', 'Max', 'Min', 'Std', 'Median'])\n",
    "X_df.to_csv('output_features.csv', index=False)  # index=Falseでインデックスを保存しない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def load_and_preprocess_data(folder_path):\n",
    "    # f0_statistics_1.csv から f0_statistics_5000.csv までのファイルを指定して読み込む\n",
    "    csv_files = [f\"{folder_path}/f0_statistics_{i}.csv\" for i in range(1, 5001)]\n",
    "    \n",
    "    # 音声ごとに特徴量とラベルを保存するリスト\n",
    "    audio_data = {}\n",
    "\n",
    "    for file in csv_files:\n",
    "        # CSVを読み込む\n",
    "        df = pd.read_csv(file)\n",
    "        # 必要な列を取得\n",
    "        data = df[['Mean', 'Max', 'Min', 'Std', 'Median', 'Accent']].values\n",
    "        \n",
    "        # 音声のIDをファイル名から取得\n",
    "        audio_id = os.path.basename(file).split('.')[0]  # 例: 'f0_statistics_1'\n",
    "        \n",
    "        # 音声ごとに特徴量とラベルを保存\n",
    "        audio_data[audio_id] = data\n",
    "\n",
    "    return audio_data\n",
    "\n",
    "# データのロード\n",
    "audio_data = load_and_preprocess_data('f0_stac_result_5000')\n",
    "\n",
    "# 保存先のディレクトリを作成\n",
    "output_folder = 'processed_audio_data'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 各音声のデータをCSVファイルに保存\n",
    "for audio_id, data in audio_data.items():\n",
    "    # 特徴量とラベルを分ける\n",
    "    X = data[:, :-1]  # 特徴量（Mean, Max, Min, Std, Median）\n",
    "    y = data[:, -1]   # ラベル（Accent）\n",
    "    \n",
    "    # DataFrameを作成\n",
    "    df = pd.DataFrame(X, columns=['Mean', 'Max', 'Min', 'Std', 'Median'])\n",
    "    df['Accent'] = y  # アクセントラベルを追加\n",
    "    \n",
    "    # CSVファイルとして保存\n",
    "    output_file = os.path.join(output_folder, f\"{audio_id}.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# 入力；f0_stac_result_5000\n",
    "# 出力；prepared_features_dataとして保存\n",
    "# 特徴量とアクセントの統合データを特徴量だけとアクセントだけのファイルに分けた！\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# データのロードと整形を行う関数\n",
    "def load_and_preprocess_data(folder_path):\n",
    "    # 指定したフォルダー内のCSVファイルを全て取得\n",
    "    csv_files = [f\"{folder_path}/f0_statistics_{i}.csv\" for i in range(1, 5001)]\n",
    "    \n",
    "    audio_data = {}\n",
    "\n",
    "    # 各CSVファイルを読み込み\n",
    "    for file in csv_files:\n",
    "        # CSVファイルをDataFrameとして読み込み\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # 特徴量とラベルを抽出\n",
    "        data = df[['Mean', 'Max', 'Min', 'Std', 'Median', 'Accent']].values\n",
    "        \n",
    "        # ファイル名から音声IDを取得\n",
    "        audio_id = os.path.basename(file).split('.')[0]  # 例: 'f0_statistics_1'\n",
    "\n",
    "        # 音声IDをキーとしてデータを辞書に格納\n",
    "        audio_data[audio_id] = data\n",
    "\n",
    "    return audio_data\n",
    "\n",
    "# データのロード\n",
    "audio_data = load_and_preprocess_data('f0_stac_result_5000')\n",
    "\n",
    "# 特徴量とラベルに分け、PyTorchのテンソルに変換する関数\n",
    "def prepare_data(audio_data):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for audio_id, data in audio_data.items():\n",
    "        # 特徴量とラベルを分ける\n",
    "        X = data[:, :-1]  # 特徴量（Mean, Max, Min, Std, Median）\n",
    "        y = data[:, -1]   # ラベル（Accent）\n",
    "\n",
    "        # 各音声データの特徴量とラベルをリストに追加\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "\n",
    "    # リストをPyTorchのテンソルに変換\n",
    "    X_tensor = [torch.tensor(X, dtype=torch.float32) for X in X_list]\n",
    "    y_tensor = [torch.tensor(y, dtype=torch.long) for y in y_list]\n",
    "\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# 特徴量とラベルを準備する\n",
    "X_tensor, y_tensor = prepare_data(audio_data)\n",
    "\n",
    "# データの確認\n",
    "for i in range(len(X_tensor)):\n",
    "    print(f\"Audio ID: {i + 1}, Feature shape: {X_tensor[i].shape}, Label shape: {y_tensor[i].shape}\")\n",
    "\n",
    "# データを保存する（必要に応じて）\n",
    "def save_data(X_tensor, y_tensor, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for i, (X, y) in enumerate(zip(X_tensor, y_tensor)):\n",
    "        # 特徴量とラベルをCSVファイルとして保存\n",
    "        df_X = pd.DataFrame(X.numpy(), columns=['Mean', 'Max', 'Min', 'Std', 'Median'])\n",
    "        df_y = pd.DataFrame(y.numpy(), columns=['Accent'])\n",
    "        \n",
    "        df_X.to_csv(os.path.join(output_folder, f\"features_audio_{i + 1}.csv\"), index=False)\n",
    "        df_y.to_csv(os.path.join(output_folder, f\"labels_audio_{i + 1}.csv\"), index=False)\n",
    "\n",
    "# 特徴量とラベルを保存（必要に応じて）\n",
    "save_data(X_tensor, y_tensor, 'prepared_features_data')\n",
    "\n",
    "print(\"データの準備と保存が完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここから！\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# データをロードする関数\n",
    "def load_csv_data(folder_path, num_files=5000):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        # 特徴量を読み込む\n",
    "        features_file = os.path.join(folder_path, f\"features_audio_{i}.csv\")\n",
    "        labels_file = os.path.join(folder_path, f\"labels_audio_{i}.csv\")\n",
    "\n",
    "        df_X = pd.read_csv(features_file)\n",
    "        df_y = pd.read_csv(labels_file)\n",
    "\n",
    "        # DataFrameからnumpy配列に変換し、PyTorchのTensorに変換\n",
    "        X = torch.tensor(df_X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(df_y.values.flatten(), dtype=torch.long)\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "# 特徴量とラベルのロード\n",
    "folder_path = 'prepared_features_data'\n",
    "X_list, y_list = load_csv_data(folder_path)\n",
    "print(X_list)\n",
    "print(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを学習用と評価用に分ける関数\n",
    "def split_data(X_list, y_list, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_list, y_list, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# 学習データと評価データに分割\n",
    "X_train, X_test, y_train, y_test = split_data(X_list, y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # 最後のタイムステップの出力を使う\n",
    "        return out\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "input_size = 5  # 特徴量の次元数 (Mean, Max, Min, Std, Median の5つ)\n",
    "hidden_size = 64\n",
    "output_size = len(set(torch.cat(y_list).numpy()))  # ラベル（アクセント）の数\n",
    "\n",
    "# モデルのインスタンス化\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# LSTMモデル定義\n",
    "class LSTMAccentModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMAccentModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # LSTMの出力\n",
    "        out = self.fc(out)  # 全ての時間ステップの出力を使用\n",
    "        return out\n",
    "\n",
    "# モデル学習のための関数\n",
    "def train_model(model, X_train, y_train, num_epochs=20, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()  # 損失関数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(zip(X_train, y_train)):\n",
    "            # バッチ化されたデータをモデルに渡して予測を取得\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 出力とラベルの形を一致させる\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            \n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 各エポックの平均損失を出力\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(X_train):.4f}\")\n",
    "\n",
    "# モデルの初期化（ラベルが0, 1, 2の場合は output_size = 3 とする）\n",
    "input_size = 5  # 特徴量の数（Mean, Max, Min, Std, Median）\n",
    "hidden_size = 64  # 隠れ層のユニット数\n",
    "output_size = 3  # ラベルの範囲が 0, 1, 2 ならば output_size = 3\n",
    "num_layers = 1  # LSTMのレイヤー数\n",
    "\n",
    "model = LSTMAccentModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# データのロード（前に定義した `X_tensor` と `y_tensor` を使用）\n",
    "X_train = X_tensor  # 特徴量\n",
    "y_train = y_tensor  # ラベル\n",
    "\n",
    "# モデルの学習を実行\n",
    "train_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まとめ\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# データをロードする関数\n",
    "def load_csv_data(folder_path, num_files=5000):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        # 特徴量を読み込む\n",
    "        features_file = os.path.join(folder_path, f\"re_features_audio_{i}.csv\")\n",
    "        labels_file = os.path.join(folder_path, f\"re_labels_audio_{i}.csv\")\n",
    "\n",
    "        df_X = pd.read_csv(features_file)\n",
    "        df_y = pd.read_csv(labels_file)\n",
    "\n",
    "        # DataFrameからnumpy配列に変換し、PyTorchのTensorに変換\n",
    "        X = torch.tensor(df_X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(df_y.values.flatten(), dtype=torch.long)\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "# 特徴量とラベルのロード\n",
    "folder_path = 'prepared_features_data'\n",
    "X_list, y_list = load_csv_data(folder_path)\n",
    "\n",
    "# データを学習用と評価用に分ける関数\n",
    "def split_data(X_list, y_list, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_list, y_list, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# 学習データと評価データに分割\n",
    "X_train, X_test, y_train, y_test = split_data(X_list, y_list)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # 最後のタイムステップの出力を使う\n",
    "        return out\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "input_size = 5  # 特徴量の次元数 (Mean, Max, Min, Std, Median の5つ)\n",
    "hidden_size = 64\n",
    "output_size = len(set(torch.cat(y_list).numpy()))  # ラベル（アクセント）の数\n",
    "\n",
    "# モデルのインスタンス化\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# LSTMモデル定義\n",
    "class LSTMAccentModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMAccentModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # LSTMの出力\n",
    "        out = self.fc(out)  # 全ての時間ステップの出力を使用\n",
    "        return out\n",
    "\n",
    "# モデル学習のための関数\n",
    "def train_model(model, X_train, y_train, num_epochs=20, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()  # 損失関数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(zip(X_train, y_train)):\n",
    "            # バッチ化されたデータをモデルに渡して予測を取得\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 出力とラベルの形を一致させる\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            \n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 各エポックの平均損失を出力\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(X_train):.4f}\")\n",
    "\n",
    "# モデルの初期化（ラベルが0, 1, 2の場合は output_size = 3 とする）\n",
    "input_size = 5  # 特徴量の数（Mean, Max, Min, Std, Median）\n",
    "hidden_size = 64  # 隠れ層のユニット数\n",
    "output_size = 3  # ラベルの範囲が 0, 1, 2 ならば output_size = 3\n",
    "num_layers = 1  # LSTMのレイヤー数\n",
    "\n",
    "model = LSTMAccentModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# データのロード（前に定義した `X_tensor` と `y_tensor` を使用）\n",
    "X_train = X_tensor  # 特徴量\n",
    "y_train = y_tensor  # ラベル\n",
    "\n",
    "# モデルの学習を実行\n",
    "train_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# 学習過程を描画\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm  # 進捗バー表示のためにtqdmをインポート\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データをロードする関数\n",
    "def load_csv_data(folder_path, num_files=5000):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        # 特徴量を読み込む\n",
    "        features_file = os.path.join(folder_path, f\"features_audio_{i}.csv\")\n",
    "        labels_file = os.path.join(folder_path, f\"labels_audio_{i}.csv\")\n",
    "\n",
    "        df_X = pd.read_csv(features_file)\n",
    "        df_y = pd.read_csv(labels_file)\n",
    "\n",
    "        # DataFrameからnumpy配列に変換し、PyTorchのTensorに変換\n",
    "        X = torch.tensor(df_X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(df_y.values.flatten(), dtype=torch.long)\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "# 特徴量とラベルのロード\n",
    "folder_path = 'prepared_features_data'\n",
    "X_list, y_list = load_csv_data(folder_path)\n",
    "\n",
    "# データを学習用と評価用に分ける関数\n",
    "def split_data(X_list, y_list, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_list, y_list, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# 学習データと評価データに分割\n",
    "X_train, X_test, y_train, y_test = split_data(X_list, y_list)\n",
    "\n",
    "# LSTMモデル定義\n",
    "class LSTMAccentModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMAccentModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # LSTMの出力\n",
    "        out = self.fc(out)  # 全ての時間ステップの出力を使用\n",
    "        return out\n",
    "\n",
    "# モデル学習のための関数\n",
    "def train_model(model, X_train, y_train, num_epochs=20, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()  # 損失関数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    losses = []  # 各エポックの損失を記録するリスト\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):  # tqdmで進捗を表示\n",
    "        total_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(zip(X_train, y_train)):\n",
    "            # バッチ化されたデータをモデルに渡して予測を取得\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 出力とラベルの形を一致させる\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            \n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 各エポックの平均損失を保存\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        # 各エポックの平均損失を出力\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 学習の進行状況をプロット\n",
    "    plt.plot(range(1, num_epochs + 1), losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# モデルの初期化（ラベルが0, 1, 2の場合は output_size = 3 とする）\n",
    "input_size = 5  # 特徴量の数（Mean, Max, Min, Std, Median）\n",
    "hidden_size = 64  # 隠れ層のユニット数\n",
    "output_size = 3  # ラベルの範囲が 0, 1, 2 ならば output_size = 3\n",
    "num_layers = 1  # LSTMのレイヤー数\n",
    "\n",
    "model = LSTMAccentModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# データのロード（前に定義した `X_tensor` と `y_tensor` を使用）\n",
    "X_train = X_tensor  # 特徴量\n",
    "y_train = y_tensor  # ラベル\n",
    "\n",
    "# モデルの学習を実行\n",
    "train_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm  # 進捗バー表示のためにtqdmをインポート\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データをロードする関数\n",
    "def load_csv_data(folder_path, num_files=5000):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        # 特徴量を読み込む\n",
    "        features_file = os.path.join(folder_path, f\"features_audio_{i}.csv\")\n",
    "        labels_file = os.path.join(folder_path, f\"labels_audio_{i}.csv\")\n",
    "\n",
    "        df_X = pd.read_csv(features_file)\n",
    "        df_y = pd.read_csv(labels_file)\n",
    "\n",
    "        # DataFrameからnumpy配列に変換し、PyTorchのTensorに変換\n",
    "        X = torch.tensor(df_X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(df_y.values.flatten(), dtype=torch.long)\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "# 特徴量とラベルのロード\n",
    "folder_path = 'prepared_features_data'\n",
    "X_list, y_list = load_csv_data(folder_path)\n",
    "\n",
    "# データを学習用と評価用に分ける関数\n",
    "def split_data(X_list, y_list, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_list, y_list, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# 学習データと評価データに分割\n",
    "X_train, X_test, y_train, y_test = split_data(X_list, y_list)\n",
    "\n",
    "# LSTMモデル定義\n",
    "class LSTMAccentModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMAccentModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # LSTMの出力\n",
    "        out = self.fc(out)  # 全ての時間ステップの出力を使用\n",
    "        return out\n",
    "\n",
    "# モデル学習のための関数\n",
    "def train_model(model, X_train, y_train, num_epochs=20, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()  # 損失関数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    losses = []  # 各エポックの損失を記録するリスト\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # tqdmでエポック全体の進捗バーを表示（全バッチを100%とする）\n",
    "        with tqdm(total=len(X_train), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "            for i, (inputs, labels) in enumerate(zip(X_train, y_train)):\n",
    "                # バッチ化されたデータをモデルに渡して予測を取得\n",
    "                inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "                labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # 出力とラベルの形を一致させる\n",
    "                outputs = outputs.view(-1, outputs.size(-1))\n",
    "                \n",
    "                # 損失計算\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # バックプロパゲーション\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # バッチごとに進捗を更新\n",
    "                pbar.update(1)\n",
    "\n",
    "        # 各エポックの平均損失を保存\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        # 各エポックの平均損失を出力\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 学習の進行状況をプロット\n",
    "    plt.plot(range(1, num_epochs + 1), losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# モデルの初期化（ラベルが0, 1, 2の場合は output_size = 3 とする）\n",
    "input_size = 5  # 特徴量の数（Mean, Max, Min, Std, Median）\n",
    "hidden_size = 64  # 隠れ層のユニット数\n",
    "output_size = 3  # ラベルの範囲が 0, 1, 2 ならば output_size = 3\n",
    "num_layers = 1  # LSTMのレイヤー数\n",
    "\n",
    "model = LSTMAccentModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# データのロード（前に定義した `X_tensor` と `y_tensor` を使用）\n",
    "X_train = X_tensor  # 特徴量\n",
    "y_train = y_tensor  # ラベル\n",
    "\n",
    "# モデルの学習を実行\n",
    "train_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm  # 進捗バー表示のためにtqdmをインポート\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データをロードする関数\n",
    "def load_csv_data(folder_path, num_files=5000):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        # 特徴量を読み込む\n",
    "        features_file = os.path.join(folder_path, f\"features_audio_{i}.csv\")\n",
    "        labels_file = os.path.join(folder_path, f\"labels_audio_{i}.csv\")\n",
    "\n",
    "        df_X = pd.read_csv(features_file)\n",
    "        df_y = pd.read_csv(labels_file)\n",
    "\n",
    "        # DataFrameからnumpy配列に変換し、PyTorchのTensorに変換\n",
    "        X = torch.tensor(df_X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(df_y.values.flatten(), dtype=torch.long)\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "# 特徴量とラベルのロード\n",
    "folder_path = 'prepared_features_data'\n",
    "X_list, y_list = load_csv_data(folder_path)\n",
    "\n",
    "# データを学習用と評価用に分ける関数\n",
    "def split_data(X_list, y_list, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_list, y_list, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# 学習データと評価データに分割\n",
    "X_train, X_test, y_train, y_test = split_data(X_list, y_list)\n",
    "\n",
    "# LSTMモデル定義\n",
    "class LSTMAccentModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMAccentModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # LSTMの出力\n",
    "        out = self.fc(out)  # 全ての時間ステップの出力を使用\n",
    "        return out\n",
    "\n",
    "# モデル学習のための関数\n",
    "def train_model(model, X_train, y_train, num_epochs=100, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()  # 損失関数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    losses = []  # 各エポックの損失を記録するリスト\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # tqdmでエポック全体の進捗バーを表示（全バッチを100%とする）\n",
    "        with tqdm(total=len(X_train), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "            for i, (inputs, labels) in enumerate(zip(X_train, y_train)):\n",
    "                # バッチ化されたデータをモデルに渡して予測を取得\n",
    "                inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "                labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # 出力とラベルの形を一致させる\n",
    "                outputs = outputs.view(-1, outputs.size(-1))\n",
    "                \n",
    "                # 損失計算\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # バックプロパゲーション\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # バッチごとに進捗を更新\n",
    "                pbar.update(1)\n",
    "\n",
    "        # 各エポックの平均損失を保存\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        # 各エポックの平均損失を出力\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 学習の進行状況をプロット\n",
    "    plt.plot(range(1, num_epochs + 1), losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# モデルの初期化（ラベルが0, 1, 2の場合は output_size = 3 とする）\n",
    "input_size = 5  # 特徴量の数（Mean, Max, Min, Std, Median）\n",
    "hidden_size = 64  # 隠れ層のユニット数\n",
    "output_size = 3  # ラベルの範囲が 0, 1, 2 ならば output_size = 3\n",
    "num_layers = 1  # LSTMのレイヤー数\n",
    "\n",
    "model = LSTMAccentModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# データのロード（前に定義した `X_tensor` と `y_tensor` を使用）\n",
    "X_train = X_tensor  # 特徴量\n",
    "y_train = y_tensor  # ラベル\n",
    "\n",
    "# モデルの学習を実行\n",
    "train_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm  # 進捗バー表示のためにtqdmをインポート\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データをロードする関数\n",
    "def load_csv_data(folder_path, num_files=5000):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        # 特徴量を読み込む\n",
    "        features_file = os.path.join(folder_path, f\"features_audio_{i}.csv\")\n",
    "        labels_file = os.path.join(folder_path, f\"labels_audio_{i}.csv\")\n",
    "\n",
    "        df_X = pd.read_csv(features_file)\n",
    "        df_y = pd.read_csv(labels_file)\n",
    "\n",
    "        # DataFrameからnumpy配列に変換し、PyTorchのTensorに変換\n",
    "        X = torch.tensor(df_X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(df_y.values.flatten(), dtype=torch.long)\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "# 特徴量とラベルのロード\n",
    "folder_path = 'prepared_features_data'\n",
    "X_list, y_list = load_csv_data(folder_path)\n",
    "\n",
    "# データを学習用と評価用に分ける関数\n",
    "def split_data(X_list, y_list, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_list, y_list, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# 学習データと評価データに分割\n",
    "X_train, X_test, y_train, y_test = split_data(X_list, y_list)\n",
    "\n",
    "# LSTMモデル定義\n",
    "class LSTMAccentModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMAccentModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # LSTMの出力\n",
    "        out = self.fc(out)  # 全ての時間ステップの出力を使用\n",
    "        return out\n",
    "\n",
    "# モデル学習のための関数\n",
    "def train_model(model, X_train, y_train, num_epochs=200, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()  # 損失関数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    losses = []  # 各エポックの損失を記録するリスト\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # tqdmでエポック全体の進捗バーを表示（全バッチを100%とする）\n",
    "        with tqdm(total=len(X_train), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "            for i, (inputs, labels) in enumerate(zip(X_train, y_train)):\n",
    "                # バッチ化されたデータをモデルに渡して予測を取得\n",
    "                inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "                labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # 出力とラベルの形を一致させる\n",
    "                outputs = outputs.view(-1, outputs.size(-1))\n",
    "                \n",
    "                # 損失計算\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # バックプロパゲーション\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # バッチごとに進捗を更新\n",
    "                pbar.update(1)\n",
    "\n",
    "        # 各エポックの平均損失を保存\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        # 各エポックの平均損失を出力\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 学習の進行状況をプロット\n",
    "    plt.plot(range(1, num_epochs + 1), losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# モデルの初期化（ラベルが0, 1, 2の場合は output_size = 3 とする）\n",
    "input_size = 5  # 特徴量の数（Mean, Max, Min, Std, Median）\n",
    "hidden_size = 64  # 隠れ層のユニット数\n",
    "output_size = 3  # ラベルの範囲が 0, 1, 2 ならば output_size = 3\n",
    "num_layers = 1  # LSTMのレイヤー数\n",
    "\n",
    "model = LSTMAccentModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# データのロード（前に定義した `X_tensor` と `y_tensor` を使用）\n",
    "X_train = X_tensor  # 特徴量\n",
    "y_train = y_tensor  # ラベル\n",
    "\n",
    "# モデルの学習を実行\n",
    "train_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己教師あり学習\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm  # 進捗バー表示のためにtqdmをインポート\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データをロードする関数\n",
    "def load_csv_data(folder_path, num_files=5000):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        # 特徴量を読み込む\n",
    "        features_file = os.path.join(folder_path, f\"features_audio_{i}.csv\")\n",
    "        labels_file = os.path.join(folder_path, f\"labels_audio_{i}.csv\")\n",
    "\n",
    "        df_X = pd.read_csv(features_file)\n",
    "        df_y = pd.read_csv(labels_file)\n",
    "\n",
    "        # DataFrameからnumpy配列に変換し、PyTorchのTensorに変換\n",
    "        X = torch.tensor(df_X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(df_y.values.flatten(), dtype=torch.long)\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "# 特徴量とラベルのロード\n",
    "folder_path = 'prepared_features_data'\n",
    "X_list, y_list = load_csv_data(folder_path)\n",
    "\n",
    "# データを学習用と評価用に分ける関数\n",
    "def split_data(X_list, y_list, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_list, y_list, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # ラベルなしデータの準備\n",
    "    X_unlabeled = X_test  # 評価用データの一部をラベルなしデータとして利用\n",
    "    return X_train, y_train, X_unlabeled\n",
    "\n",
    "# 学習データ、評価データ、ラベルなしデータに分割\n",
    "X_train, y_train, X_unlabeled = split_data(X_list, y_list)\n",
    "\n",
    "# LSTMモデル定義\n",
    "class LSTMAccentModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMAccentModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 自己教師あり学習用のモデルを定義\n",
    "class SelfSupervisedModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SelfSupervisedModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)  # 入力と同じ次元に戻す\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 自己教師あり学習モデルの初期化\n",
    "self_supervised_model = SelfSupervisedModel(input_size=5, hidden_size=64)  # 特徴量の数に応じて調整\n",
    "\n",
    "# 自己教師あり学習のトレーニングループの修正\n",
    "def train_self_supervised_model(model, X_unlabeled, num_epochs=100, learning_rate=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()  # 入力を再構築するための損失関数\n",
    "    loss_history = []  # 損失を記録するリストを関数内で定義\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0  # エポックごとの累積損失\n",
    "        for inputs in X_unlabeled:\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            outputs = model(inputs)  # モデルの出力を取得\n",
    "\n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, inputs)\n",
    "            total_loss += loss.item()  # 累積損失を加算\n",
    "            \n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_unlabeled)  # 平均損失を計算\n",
    "        loss_history.append(avg_loss)  # 平均損失を記録\n",
    "        print(f\"Self-supervised Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 損失のプロット\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Self-supervised Learning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# 自己教師あり学習の実行\n",
    "train_self_supervised_model(self_supervised_model, X_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングのためのLSTMモデルの初期化\n",
    "model = LSTMAccentModel(input_size=5, hidden_size=64, output_size=3)  # ラベルが0, 1, 2の場合は output_size = 3 とする\n",
    "\n",
    "# ファインチューニングの実行\n",
    "def fine_tune_model(model, X_train, y_train, num_epochs=100, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []  # 損失を記録するリストを定義\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in zip(X_train, y_train):\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        loss_history.append(avg_loss)  # 平均損失を記録\n",
    "        print(f\"Fine-tuning Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 損失のプロット\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Fine-tuning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# ファインチューニングの実行\n",
    "fine_tune_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己教師あり学習\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm  # 進捗バー表示のためにtqdmをインポート\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データをロードする関数\n",
    "def load_csv_data(folder_path, num_files=5000):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        # 特徴量を読み込む\n",
    "        features_file = os.path.join(folder_path, f\"features_audio_{i}.csv\")\n",
    "        labels_file = os.path.join(folder_path, f\"labels_audio_{i}.csv\")\n",
    "\n",
    "        df_X = pd.read_csv(features_file)\n",
    "        df_y = pd.read_csv(labels_file)\n",
    "\n",
    "        # DataFrameからnumpy配列に変換し、PyTorchのTensorに変換\n",
    "        X = torch.tensor(df_X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(df_y.values.flatten(), dtype=torch.long)\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "# 特徴量とラベルのロード\n",
    "folder_path = 'prepared_features_data'\n",
    "X_list, y_list = load_csv_data(folder_path)\n",
    "\n",
    "# データを学習用と評価用に分ける関数\n",
    "def split_data(X_list, y_list, test_size=0.8, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_list, y_list, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # ラベルなしデータの準備\n",
    "    X_unlabeled = X_test  # 評価用データの一部をラベルなしデータとして利用\n",
    "    return X_train, y_train, X_unlabeled\n",
    "\n",
    "# 学習データ、評価データ、ラベルなしデータに分割\n",
    "X_train, y_train, X_unlabeled = split_data(X_list, y_list)\n",
    "\n",
    "# LSTMモデル定義\n",
    "class LSTMAccentModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMAccentModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 自己教師あり学習用のモデルを定義\n",
    "class SelfSupervisedModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SelfSupervisedModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)  # 入力と同じ次元に戻す\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 自己教師あり学習モデルの初期化\n",
    "self_supervised_model = SelfSupervisedModel(input_size=5, hidden_size=64)  # 特徴量の数に応じて調整\n",
    "\n",
    "# 自己教師あり学習のトレーニングループの修正\n",
    "def train_self_supervised_model(model, X_unlabeled, num_epochs=100, learning_rate=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()  # 入力を再構築するための損失関数, 平均二乗誤差\n",
    "    loss_history = []  # 損失を記録するリストを関数内で定義\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0  # エポックごとの累積損失\n",
    "        for inputs in X_unlabeled:\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            outputs = model(inputs)  # モデルの出力を取得\n",
    "\n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, inputs)\n",
    "            total_loss += loss.item()  # 累積損失を加算\n",
    "            \n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_unlabeled)  # 平均損失を計算\n",
    "        loss_history.append(avg_loss)  # 平均損失を記録\n",
    "        print(f\"Self-supervised Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 損失のプロット\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Self-supervised Learning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# 自己教師あり学習の実行\n",
    "train_self_supervised_model(self_supervised_model, X_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングのためのLSTMモデルの初期化\n",
    "model = LSTMAccentModel(input_size=5, hidden_size=64, output_size=3)  # ラベルが0, 1, 2の場合は output_size = 3 とする\n",
    "\n",
    "# ファインチューニングの実行\n",
    "def fine_tune_model(model, X_train, y_train, num_epochs=100, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []  # 損失を記録するリストを定義\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in zip(X_train, y_train):\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        loss_history.append(avg_loss)  # 平均損失を記録\n",
    "        print(f\"Fine-tuning Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 損失のプロット\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Fine-tuning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# ファインチューニングの実行\n",
    "fine_tune_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングのためのLSTMモデルの初期化\n",
    "model = LSTMAccentModel(input_size=5, hidden_size=64, output_size=3)  # ラベルが0, 1, 2の場合は output_size = 3 とする\n",
    "\n",
    "# ファインチューニングの実行\n",
    "def fine_tune_model(model, X_train, y_train, num_epochs=100, learning_rate=0.0001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []  # 損失を記録するリストを定義\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in zip(X_train, y_train):\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        loss_history.append(avg_loss)  # 平均損失を記録\n",
    "        print(f\"Fine-tuning Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 損失のプロット\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Fine-tuning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# ファインチューニングの実行\n",
    "fine_tune_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングのためのLSTMモデルの初期化\n",
    "model = LSTMAccentModel(input_size=5, hidden_size=128, output_size=3)  # ラベルが0, 1, 2の場合は output_size = 3 とする\n",
    "\n",
    "# ファインチューニングの実行\n",
    "def fine_tune_model(model, X_train, y_train, num_epochs=100, learning_rate=0.0001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []  # 損失を記録するリストを定義\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in zip(X_train, y_train):\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        loss_history.append(avg_loss)  # 平均損失を記録\n",
    "        print(f\"Fine-tuning Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 損失のプロット\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Fine-tuning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# ファインチューニングの実行\n",
    "fine_tune_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングのためのLSTMモデルの初期化\n",
    "model = LSTMAccentModel(input_size=5, hidden_size=256, output_size=3)  # ラベルが0, 1, 2の場合は output_size = 3 とする\n",
    "\n",
    "# ファインチューニングの実行\n",
    "def fine_tune_model(model, X_train, y_train, num_epochs=100, learning_rate=0.0001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []  # 損失を記録するリストを定義\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in zip(X_train, y_train):\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        loss_history.append(avg_loss)  # 平均損失を記録\n",
    "        print(f\"Fine-tuning Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 損失のプロット\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Fine-tuning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# ファインチューニングの実行\n",
    "fine_tune_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングのためのLSTMモデルの初期化\n",
    "model = LSTMAccentModel(input_size=5, hidden_size=256, output_size=3)  # ラベルが0, 1, 2の場合は output_size = 3 とする\n",
    "\n",
    "# ファインチューニングの実行\n",
    "def fine_tune_model(model, X_train, y_train, num_epochs=120, learning_rate=0.0001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []  # 損失を記録するリストを定義\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in zip(X_train, y_train):\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        loss_history.append(avg_loss)  # 平均損失を記録\n",
    "        print(f\"Fine-tuning Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 損失のプロット\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Fine-tuning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# ファインチューニングの実行\n",
    "fine_tune_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングのためのLSTMモデルの初期化\n",
    "model = LSTMAccentModel(input_size=5, hidden_size=256, output_size=3)  # ラベルが0, 1, 2の場合は output_size = 3 とする\n",
    "\n",
    "# ファインチューニングの実行\n",
    "def fine_tune_model(model, X_train, y_train, num_epochs=140, learning_rate=0.0001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []  # 損失を記録するリストを定義\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in zip(X_train, y_train):\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        loss_history.append(avg_loss)  # 平均損失を記録\n",
    "        print(f\"Fine-tuning Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 損失のプロット\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Fine-tuning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# ファインチューニングの実行\n",
    "fine_tune_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングのためのLSTMモデルの初期化\n",
    "model = LSTMAccentModel(input_size=5, hidden_size=256, output_size=3)  # ラベルが0, 1, 2の場合は output_size = 3 とする\n",
    "\n",
    "# ファインチューニングの実行\n",
    "def fine_tune_model(model, X_train, y_train, num_epochs=180, learning_rate=0.0001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []  # 損失を記録するリストを定義\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in zip(X_train, y_train):\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        loss_history.append(avg_loss)  # 平均損失を記録\n",
    "        print(f\"Fine-tuning Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 損失のプロット\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Fine-tuning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# ファインチューニングの実行\n",
    "fine_tune_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングのためのLSTMモデルの初期化\n",
    "model = LSTMAccentModel(input_size=5, hidden_size=256, output_size=3)  # ラベルが0, 1, 2の場合は output_size = 3 とする\n",
    "\n",
    "# ファインチューニングの実行\n",
    "def fine_tune_model(model, X_train, y_train, num_epochs=200, learning_rate=0.0001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []  # 損失を記録するリストを定義\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in zip(X_train, y_train):\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        loss_history.append(avg_loss)  # 平均損失を記録\n",
    "        print(f\"Fine-tuning Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 損失のプロット\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Fine-tuning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# ファインチューニングの実行\n",
    "fine_tune_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングのためのLSTMモデルの初期化\n",
    "model = LSTMAccentModel(input_size=5, hidden_size=256, output_size=3)  # ラベルが0, 1, 2の場合は output_size = 3 とする\n",
    "\n",
    "# ファインチューニングの実行\n",
    "def fine_tune_model(model, X_train, y_train, num_epochs=300, learning_rate=0.0001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []  # 損失を記録するリストを定義\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in zip(X_train, y_train):\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        loss_history.append(avg_loss)  # 平均損失を記録\n",
    "        print(f\"Fine-tuning Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 損失のプロット\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Fine-tuning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# ファインチューニングの実行\n",
    "fine_tune_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己学習用；3000個、ファインチューニング用；1000個、評価用；1000個\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データをロードする関数\n",
    "def load_csv_data(folder_path, num_files=5000):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        features_file = os.path.join(folder_path, f\"features_audio_{i}.csv\")\n",
    "        labels_file = os.path.join(folder_path, f\"labels_audio_{i}.csv\")\n",
    "\n",
    "        df_X = pd.read_csv(features_file)\n",
    "        df_y = pd.read_csv(labels_file)\n",
    "\n",
    "        X = torch.tensor(df_X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(df_y.values.flatten(), dtype=torch.long)\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "# 特徴量とラベルのロード\n",
    "folder_path = 'prepared_features_data'\n",
    "X_list, y_list = load_csv_data(folder_path)\n",
    "\n",
    "# データを学習用、評価用、ラベルなし用に分ける関数\n",
    "def split_data(X_list, y_list, test_size=0.4, random_state=42):\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X_list, y_list, test_size=test_size, random_state=random_state)\n",
    "    X_train, X_unlabeled, y_train, _ = train_test_split(X_temp, y_temp, test_size=0.5, random_state=random_state)\n",
    "    return X_train, y_train, X_test, y_test, X_unlabeled\n",
    "\n",
    "# 学習データ、評価データ、ラベルなしデータに分割\n",
    "X_train, y_train, X_test, y_test, X_unlabeled = split_data(X_list, y_list)\n",
    "\n",
    "# LSTMモデル定義\n",
    "class LSTMAccentModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMAccentModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 自己教師あり学習用のモデルを定義\n",
    "class SelfSupervisedModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SelfSupervisedModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 自己教師あり学習モデルの初期化\n",
    "self_supervised_model = SelfSupervisedModel(input_size=5, hidden_size=64)\n",
    "\n",
    "# 自己教師あり学習のトレーニングループ\n",
    "def train_self_supervised_model(model, X_unlabeled, num_epochs=100, learning_rate=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs in X_unlabeled:\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, inputs)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_unlabeled)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"Self-supervised Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Self-supervised Learning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# 自己教師あり学習の実行\n",
    "train_self_supervised_model(self_supervised_model, X_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングのためのLSTMモデルの初期化\n",
    "model = LSTMAccentModel(input_size=5, hidden_size=56, output_size=3)\n",
    "\n",
    "# ファインチューニングの実行\n",
    "def fine_tune_model(model, X_train, y_train, num_epochs=100, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in zip(X_train, y_train):\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"Fine-tuning Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Fine-tuning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# ファインチューニングの実行\n",
    "fine_tune_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 評価関数の定義\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()  # 評価モードに切り替え\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():  # 勾配計算を無効に\n",
    "        for inputs in X_test:\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 2)  # 最も高いスコアを持つクラスを選択\n",
    "            predictions.append(predicted.view(-1))\n",
    "\n",
    "    # すべての予測を連結\n",
    "    predicted_tensor = torch.cat(predictions)\n",
    "    \n",
    "    # y_testを連結\n",
    "    y_test_tensor = torch.cat(y_test)\n",
    "    \n",
    "    # 正解率を計算\n",
    "    accuracy = accuracy_score(y_test_tensor.numpy(), predicted_tensor.numpy())\n",
    "    return accuracy, y_test_tensor, predicted_tensor\n",
    "\n",
    "# 評価の実行\n",
    "accuracy, y_test_tensor, predicted_tensor = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "# 一致率を出力\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果の保存\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 保存するフォルダの作成\n",
    "output_dir = 'predicted_accents'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 評価関数の修正: 予測結果をCSVファイルに保存\n",
    "def save_predictions_to_csv(model, X_test, y_test, output_dir):\n",
    "    model.eval()  # 評価モードに切り替え\n",
    "\n",
    "    with torch.no_grad():  # 勾配計算を無効に\n",
    "        for i, inputs in enumerate(X_test):\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 2)  # 最も高いスコアを持つクラスを選択\n",
    "            \n",
    "            # 予測結果をTensorからNumPy配列に変換\n",
    "            predicted_np = predicted.view(-1).numpy()\n",
    "            \n",
    "            # CSVファイルに保存\n",
    "            file_name = os.path.join(output_dir, f'predicted_accent_{i+1}.csv')\n",
    "            pd.DataFrame(predicted_np, columns=['Predicted Accent']).to_csv(file_name, index=False)\n",
    "\n",
    "# 予測結果を保存\n",
    "save_predictions_to_csv(model, X_test, y_test, output_dir)\n",
    "\n",
    "print(f\"Predicted accents saved in the '{output_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# フォルダのパスを指定\n",
    "predicted_dir = 'predicted_accents'\n",
    "\n",
    "# y_testをリストに変換する関数\n",
    "def load_y_test_tensor(y_test):\n",
    "    # ここでは、y_testがTensorのリストであると仮定します\n",
    "    return [tensor.numpy() for tensor in y_test]  # TensorをNumPy配列に変換してリストに追加\n",
    "\n",
    "# y_testをリスト形式でロード\n",
    "y_test_list = load_y_test_tensor(y_test)\n",
    "\n",
    "# 一致数をカウントするための変数を初期化\n",
    "total_matches = 0\n",
    "total_samples = 0\n",
    "\n",
    "# 予測結果をロードして一致をカウント\n",
    "for i in range(1, 1001):  # predicted_accent_1.csv から predicted_accent_1000.csv まで\n",
    "    file_name = os.path.join(predicted_dir, f'predicted_accent_{i}.csv')\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        # CSVファイルを読み込む\n",
    "        predicted_df = pd.read_csv(file_name)\n",
    "        predicted_array = predicted_df['Predicted Accent'].to_numpy()\n",
    "\n",
    "        # 一致数をカウント\n",
    "        if i <= len(y_test_list):  # y_testの範囲内であるか確認\n",
    "            total_matches += (predicted_array == y_test_list[i - 1]).sum()\n",
    "            total_samples += len(predicted_array)\n",
    "\n",
    "# 一致率を計算\n",
    "if total_samples > 0:\n",
    "    accuracy = total_matches / total_samples\n",
    "    print(f'一致数: {total_matches} / 総サンプル数: {total_samples} / 一致率: {accuracy:.4f}')\n",
    "else:\n",
    "    print('サンプルが存在しません。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# フォルダのパスを指定\n",
    "predicted_dir = 'predicted_accents'\n",
    "\n",
    "# y_testをリストに変換する関数\n",
    "def load_y_test_tensor(y_test):\n",
    "    return [tensor.numpy() for tensor in y_test]  # TensorをNumPy配列に変換してリストに追加\n",
    "\n",
    "# y_testをリスト形式でロード\n",
    "y_test_list = load_y_test_tensor(y_test)\n",
    "\n",
    "# 一致数をカウントするための変数を初期化\n",
    "total_matches = 0\n",
    "total_samples = 0\n",
    "match_class_1 = 0\n",
    "match_class_2 = 0\n",
    "\n",
    "# 予測結果をロードして一致をカウント\n",
    "for i in range(1, 1001):  # predicted_accent_1.csv から predicted_accent_1000.csv まで\n",
    "    file_name = os.path.join(predicted_dir, f'predicted_accent_{i}.csv')\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        # CSVファイルを読み込む\n",
    "        predicted_df = pd.read_csv(file_name)\n",
    "        predicted_array = predicted_df['Predicted Accent'].to_numpy()\n",
    "\n",
    "        # 一致数をカウント\n",
    "        if i <= len(y_test_list):  # y_testの範囲内であるか確認\n",
    "            total_matches += (predicted_array == y_test_list[i - 1]).sum()\n",
    "            total_samples += len(predicted_array)\n",
    "\n",
    "            # クラス1の一致数\n",
    "            match_class_1 += ((predicted_array == 1) & (y_test_list[i - 1] == 1)).sum()\n",
    "\n",
    "            # クラス2の一致数\n",
    "            match_class_2 += ((predicted_array == 2) & (y_test_list[i - 1] == 2)).sum()\n",
    "\n",
    "# 一致率を計算\n",
    "if total_samples > 0:\n",
    "    accuracy = total_matches / total_samples\n",
    "    print(f'総一致数: {total_matches} / クラス1の一致数: {match_class_1} / クラス2の一致数: {match_class_2} / 総サンプル数: {total_samples} / 一致率: {accuracy:.4f}')\n",
    "else:\n",
    "    print('サンプルが存在しません。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# フォルダのパスを指定\n",
    "predicted_dir = 'predicted_accents'\n",
    "\n",
    "# y_testをリストに変換する関数\n",
    "def load_y_test_tensor(y_test):\n",
    "    return [tensor.numpy() for tensor in y_test]  # TensorをNumPy配列に変換してリストに追加\n",
    "\n",
    "# y_testをリスト形式でロード\n",
    "y_test_list = load_y_test_tensor(y_test)\n",
    "\n",
    "# 一致数をカウントするための変数を初期化\n",
    "total_matches = 0\n",
    "total_samples = 0\n",
    "match_class_0 = 0\n",
    "match_class_1 = 0\n",
    "match_class_2 = 0\n",
    "\n",
    "# 各クラスの総数をカウントするための変数を初期化\n",
    "total_class_0 = 0\n",
    "total_class_1 = 0\n",
    "total_class_2 = 0\n",
    "\n",
    "# 予測結果をロードして一致をカウント\n",
    "for i in range(1, 1001):  # predicted_accent_1.csv から predicted_accent_1000.csv まで\n",
    "    file_name = os.path.join(predicted_dir, f'predicted_accent_{i}.csv')\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        # CSVファイルを読み込む\n",
    "        predicted_df = pd.read_csv(file_name)\n",
    "        predicted_array = predicted_df['Predicted Accent'].to_numpy()\n",
    "\n",
    "        # 一致数をカウント\n",
    "        if i <= len(y_test_list):  # y_testの範囲内であるか確認\n",
    "            total_matches += (predicted_array == y_test_list[i - 1]).sum()\n",
    "            total_samples += len(predicted_array)\n",
    "\n",
    "            # 各クラスの一致数と総数をカウント\n",
    "            match_class_0 += ((predicted_array == 0) & (y_test_list[i - 1] == 0)).sum()\n",
    "            match_class_1 += ((predicted_array == 1) & (y_test_list[i - 1] == 1)).sum()\n",
    "            match_class_2 += ((predicted_array == 2) & (y_test_list[i - 1] == 2)).sum()\n",
    "\n",
    "            total_class_0 += (y_test_list[i - 1] == 0).sum()\n",
    "            total_class_1 += (y_test_list[i - 1] == 1).sum()\n",
    "            total_class_2 += (y_test_list[i - 1] == 2).sum()\n",
    "\n",
    "# 各クラスの一致率を計算\n",
    "accuracy_class_0 = match_class_0 / total_class_0 if total_class_0 > 0 else 0\n",
    "accuracy_class_1 = match_class_1 / total_class_1 if total_class_1 > 0 else 0\n",
    "accuracy_class_2 = match_class_2 / total_class_2 if total_class_2 > 0 else 0\n",
    "\n",
    "# 結果を出力\n",
    "print(f'クラス0の一致数: {match_class_0} / クラス0の総数: {total_class_0} / 一致率: {match_class_0}/{total_class_0} = {accuracy_class_0:.4f}')\n",
    "print(f'クラス1の一致数: {match_class_1} / クラス1の総数: {total_class_1} / 一致率: {match_class_1}/{total_class_1} = {accuracy_class_1:.4f}')\n",
    "print(f'クラス2の一致数: {match_class_2} / クラス2の総数: {total_class_2} / 一致率: {match_class_2}/{total_class_2} = {accuracy_class_2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# アクセントを高低(1, 0)の2値に変更\n",
    "# 入力；re_f0_stac_result_5000\n",
    "# 出力；re_prepared_features_dataとして保存\n",
    "# 特徴量とアクセントの統合データを特徴量だけとアクセントだけのファイルに分けた！\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# データのロードと整形を行う関数\n",
    "def load_and_preprocess_data(folder_path):\n",
    "    # 指定したフォルダー内のCSVファイルを全て取得\n",
    "    csv_files = [f\"{folder_path}/re_f0_statistics_{i}.csv\" for i in range(1, 5001)]\n",
    "    \n",
    "    audio_data = {}\n",
    "\n",
    "    # 各CSVファイルを読み込み\n",
    "    for file in csv_files:\n",
    "        # CSVファイルをDataFrameとして読み込み\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # 特徴量とラベルを抽出\n",
    "        data = df[['Mean', 'Max', 'Min', 'Std', 'Median', 'Accent']].values\n",
    "        \n",
    "        # ファイル名から音声IDを取得\n",
    "        audio_id = os.path.basename(file).split('.')[0]  # 例: 'f0_statistics_1'\n",
    "\n",
    "        # 音声IDをキーとしてデータを辞書に格納\n",
    "        audio_data[audio_id] = data\n",
    "\n",
    "    return audio_data\n",
    "\n",
    "# データのロード\n",
    "audio_data = load_and_preprocess_data('re_f0_stac_result_5000')\n",
    "\n",
    "# 特徴量とラベルに分け、PyTorchのテンソルに変換する関数\n",
    "def prepare_data(audio_data):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for audio_id, data in audio_data.items():\n",
    "        # 特徴量とラベルを分ける\n",
    "        X = data[:, :-1]  # 特徴量（Mean, Max, Min, Std, Median）\n",
    "        y = data[:, -1]   # ラベル（Accent）\n",
    "\n",
    "        # 各音声データの特徴量とラベルをリストに追加\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "\n",
    "    # リストをPyTorchのテンソルに変換\n",
    "    X_tensor = [torch.tensor(X, dtype=torch.float32) for X in X_list]\n",
    "    y_tensor = [torch.tensor(y, dtype=torch.long) for y in y_list]\n",
    "\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# 特徴量とラベルを準備する\n",
    "X_tensor, y_tensor = prepare_data(audio_data)\n",
    "\n",
    "# データの確認\n",
    "for i in range(len(X_tensor)):\n",
    "    print(f\"Audio ID: {i + 1}, Feature shape: {X_tensor[i].shape}, Label shape: {y_tensor[i].shape}\")\n",
    "\n",
    "# データを保存する（必要に応じて）\n",
    "def save_data(X_tensor, y_tensor, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for i, (X, y) in enumerate(zip(X_tensor, y_tensor)):\n",
    "        # 特徴量とラベルをCSVファイルとして保存\n",
    "        df_X = pd.DataFrame(X.numpy(), columns=['Mean', 'Max', 'Min', 'Std', 'Median'])\n",
    "        df_y = pd.DataFrame(y.numpy(), columns=['Accent'])\n",
    "        \n",
    "        df_X.to_csv(os.path.join(output_folder, f\"re_features_audio_{i + 1}.csv\"), index=False)\n",
    "        df_y.to_csv(os.path.join(output_folder, f\"re_labels_audio_{i + 1}.csv\"), index=False)\n",
    "\n",
    "# 特徴量とラベルを保存（必要に応じて）\n",
    "save_data(X_tensor, y_tensor, 're_prepared_features_data')\n",
    "\n",
    "print(\"データの準備と保存が完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高低（2→1, 1→0）に変換\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm  # 進捗バー表示のためにtqdmをインポート\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データをロードする関数\n",
    "def load_csv_data(folder_path, num_files=5000):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        # 特徴量を読み込む\n",
    "        features_file = os.path.join(folder_path, f\"re_features_audio_{i}.csv\")\n",
    "        labels_file = os.path.join(folder_path, f\"re_labels_audio_{i}.csv\")\n",
    "\n",
    "        df_X = pd.read_csv(features_file)\n",
    "        df_y = pd.read_csv(labels_file)\n",
    "\n",
    "        # DataFrameからnumpy配列に変換し、PyTorchのTensorに変換\n",
    "        X = torch.tensor(df_X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(df_y.values.flatten(), dtype=torch.long)\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "# 特徴量とラベルのロード\n",
    "folder_path = 're_prepared_features_data'\n",
    "X_list, y_list = load_csv_data(folder_path)\n",
    "\n",
    "# データを学習用と評価用に分ける関数\n",
    "def split_data(X_list, y_list, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_list, y_list, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# 学習データと評価データに分割\n",
    "X_train, X_test, y_train, y_test = split_data(X_list, y_list)\n",
    "\n",
    "# LSTMモデル定義\n",
    "class LSTMAccentModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMAccentModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # LSTMの出力\n",
    "        out = self.fc(out)  # 全ての時間ステップの出力を使用\n",
    "        return out\n",
    "\n",
    "# モデル学習のための関数\n",
    "def train_model(model, X_train, y_train, num_epochs=20, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()  # 損失関数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    losses = []  # 各エポックの損失を記録するリスト\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):  # tqdmで進捗を表示\n",
    "        total_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(zip(X_train, y_train)):\n",
    "            # バッチ化されたデータをモデルに渡して予測を取得\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 出力とラベルの形を一致させる\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            \n",
    "            # 損失計算\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 各エポックの平均損失を保存\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        # 各エポックの平均損失を出力\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 学習の進行状況をプロット\n",
    "    plt.plot(range(1, num_epochs + 1), losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# モデルの初期化（ラベルが0, 1の場合は output_size = 2 とする）\n",
    "input_size = 5  # 特徴量の数（Mean, Max, Min, Std, Median）\n",
    "hidden_size = 64  # 隠れ層のユニット数\n",
    "output_size = 2  # ラベルの範囲が 0, 1 ならば output_size = 2\n",
    "num_layers = 1  # LSTMのレイヤー数\n",
    "\n",
    "model = LSTMAccentModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# モデルの学習を実行\n",
    "train_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm  # 進捗バー表示のためにtqdmをインポート\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データをロードする関数\n",
    "def load_csv_data(folder_path, num_files=5000):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        # 特徴量を読み込む\n",
    "        features_file = os.path.join(folder_path, f\"re_features_audio_{i}.csv\")\n",
    "        labels_file = os.path.join(folder_path, f\"re_labels_audio_{i}.csv\")\n",
    "\n",
    "        df_X = pd.read_csv(features_file)\n",
    "        df_y = pd.read_csv(labels_file)\n",
    "\n",
    "        # DataFrameからnumpy配列に変換し、PyTorchのTensorに変換\n",
    "        X = torch.tensor(df_X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(df_y.values.flatten(), dtype=torch.long)\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "# 特徴量とラベルのロード\n",
    "folder_path = 're_prepared_features_data'\n",
    "X_list, y_list = load_csv_data(folder_path)\n",
    "\n",
    "# データを学習用と評価用に分ける関数\n",
    "def split_data(X_list, y_list, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_list, y_list, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# 学習データと評価データに分割\n",
    "X_train, X_test, y_train, y_test = split_data(X_list, y_list)\n",
    "\n",
    "# LSTMモデル定義\n",
    "class LSTMAccentModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMAccentModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # LSTMの出力\n",
    "        out = self.fc(out)  # 全ての時間ステップの出力を使用\n",
    "        return out\n",
    "\n",
    "# モデル学習のための関数\n",
    "def train_model(model, X_train, y_train, num_epochs=100, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()  # 損失関数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    losses = []  # 各エポックの損失を記録するリスト\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # tqdmでエポック全体の進捗バーを表示（全バッチを100%とする）\n",
    "        with tqdm(total=len(X_train), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "            for i, (inputs, labels) in enumerate(zip(X_train, y_train)):\n",
    "                # バッチ化されたデータをモデルに渡して予測を取得\n",
    "                inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "                labels = labels.view(-1)  # ラベルを1次元に変換\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # 出力とラベルの形を一致させる\n",
    "                outputs = outputs.view(-1, outputs.size(-1))\n",
    "                \n",
    "                # 損失計算\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # バックプロパゲーション\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # バッチごとに進捗を更新\n",
    "                pbar.update(1)\n",
    "\n",
    "        # 各エポックの平均損失を保存\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        # 各エポックの平均損失を出力\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 学習の進行状況をプロット\n",
    "    plt.plot(range(1, num_epochs + 1), losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# モデルの初期化（ラベルが0, 1の場合は output_size = 2 とする）\n",
    "input_size = 5  # 特徴量の数（Mean, Max, Min, Std, Median）\n",
    "hidden_size = 64  # 隠れ層のユニット数\n",
    "output_size = 2  # ラベルの範囲が 0, 1 ならば output_size = 2\n",
    "num_layers = 1  # LSTMのレイヤー数\n",
    "\n",
    "model = LSTMAccentModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# モデルの学習を実行\n",
    "train_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用：3500, ファインチューニング用：750, 評価用；750\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データをロードする関数\n",
    "def load_csv_data(folder_path, num_files=5000):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        features_file = os.path.join(folder_path, f\"re_features_audio_{i}.csv\")\n",
    "        labels_file = os.path.join(folder_path, f\"re_labels_audio_{i}.csv\")\n",
    "\n",
    "        df_X = pd.read_csv(features_file)\n",
    "        df_y = pd.read_csv(labels_file)\n",
    "\n",
    "        X = torch.tensor(df_X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(df_y.values.flatten(), dtype=torch.long)\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "# 特徴量とラベルのロード\n",
    "folder_path = 're_prepared_features_data'\n",
    "X_list, y_list = load_csv_data(folder_path)\n",
    "\n",
    "# データを学習用、ファインチューニング用、評価用に分ける関数\n",
    "def split_data(X_list, y_list, train_size=0.7, val_size=0.15, random_state=42):\n",
    "    # 学習用データと残りを分割\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_list, y_list, test_size=(1 - train_size), random_state=random_state)\n",
    "\n",
    "    # 残りのデータをファインチューニング用と評価用に分割\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(0.5), random_state=random_state)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# 学習データ、ファインチューニングデータ、評価データに分割\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(X_list, y_list, train_size=3500/5000, val_size=750/5000)\n",
    "\n",
    "# LSTMモデル定義\n",
    "class LSTMAccentModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMAccentModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 自己教師あり学習用のモデルを定義\n",
    "class SelfSupervisedModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SelfSupervisedModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 自己教師あり学習モデルの初期化\n",
    "self_supervised_model = SelfSupervisedModel(input_size=5, hidden_size=64)\n",
    "\n",
    "# 自己教師あり学習のトレーニングループ\n",
    "def train_self_supervised_model(model, X_train, num_epochs=100, learning_rate=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs in X_train:\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, inputs)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"Self-supervised Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Self-supervised Learning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# 自己教師あり学習の実行\n",
    "train_self_supervised_model(self_supervised_model, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングのためのLSTMモデルの初期化\n",
    "model = LSTMAccentModel(input_size=5, hidden_size=56, output_size=2)\n",
    "\n",
    "# ファインチューニングの実行\n",
    "def fine_tune_model(model, X_val, y_val, num_epochs=100, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in zip(X_val, y_val):\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_val)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"Fine-tuning Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Fine-tuning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# ファインチューニングの実行\n",
    "fine_tune_model(model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 評価関数の定義\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()  # 評価モードに切り替え\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():  # 勾配計算を無効に\n",
    "        for inputs in X_test:\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 2)  # 最も高いスコアを持つクラスを選択\n",
    "            predictions.append(predicted.view(-1))\n",
    "\n",
    "    # すべての予測を連結\n",
    "    predicted_tensor = torch.cat(predictions)\n",
    "    \n",
    "    # y_testを連結\n",
    "    y_test_tensor = torch.cat(y_test)\n",
    "    \n",
    "    # 正解率を計算\n",
    "    accuracy = accuracy_score(y_test_tensor.numpy(), predicted_tensor.numpy())\n",
    "    return accuracy, y_test_tensor, predicted_tensor\n",
    "\n",
    "# 評価の実行\n",
    "accuracy, y_test_tensor, predicted_tensor = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "# 一致率を出力\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果の保存\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 保存するフォルダの作成\n",
    "output_dir = 're_predicted_accents'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 評価関数の修正: 予測結果をCSVファイルに保存\n",
    "def save_predictions_to_csv(model, X_test, y_test, output_dir):\n",
    "    model.eval()  # 評価モードに切り替え\n",
    "\n",
    "    with torch.no_grad():  # 勾配計算を無効に\n",
    "        for i, inputs in enumerate(X_test):\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 2)  # 最も高いスコアを持つクラスを選択\n",
    "            \n",
    "            # 予測結果をTensorからNumPy配列に変換\n",
    "            predicted_np = predicted.view(-1).numpy()\n",
    "            \n",
    "            # CSVファイルに保存\n",
    "            file_name = os.path.join(output_dir, f're_predicted_accent_{i+1}.csv')\n",
    "            pd.DataFrame(predicted_np, columns=['Predicted Accent']).to_csv(file_name, index=False)\n",
    "\n",
    "# 予測結果を保存\n",
    "save_predictions_to_csv(model, X_test, y_test, output_dir)\n",
    "\n",
    "print(f\"Predicted accents saved in the '{output_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# フォルダのパスを指定\n",
    "predicted_dir = 're_predicted_accents'\n",
    "\n",
    "# y_testをリストに変換する関数\n",
    "def load_y_test_tensor(y_test):\n",
    "    return [tensor.numpy() for tensor in y_test]  # TensorをNumPy配列に変換してリストに追加\n",
    "\n",
    "# y_testをリスト形式でロード\n",
    "y_test_list = load_y_test_tensor(y_test)\n",
    "\n",
    "# 一致数をカウントするための変数を初期化\n",
    "total_matches = 0\n",
    "total_samples = 0\n",
    "match_class_0 = 0\n",
    "match_class_1 = 0\n",
    "\n",
    "# 各クラスの総数をカウントするための変数を初期化\n",
    "total_class_0 = 0\n",
    "total_class_1 = 0\n",
    "\n",
    "# 予測結果をロードして一致をカウント\n",
    "for i in range(1, 1001):  # predicted_accent_1.csv から predicted_accent_1000.csv まで\n",
    "    file_name = os.path.join(predicted_dir, f're_predicted_accent_{i}.csv')\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        # CSVファイルを読み込む\n",
    "        predicted_df = pd.read_csv(file_name)\n",
    "        predicted_array = predicted_df['Predicted Accent'].to_numpy()\n",
    "\n",
    "        # 一致数をカウント\n",
    "        if i <= len(y_test_list):  # y_testの範囲内であるか確認\n",
    "            total_matches += (predicted_array == y_test_list[i - 1]).sum()\n",
    "            total_samples += len(predicted_array)\n",
    "\n",
    "            # クラス0とクラス1の一致数と総数をカウント\n",
    "            match_class_0 += ((predicted_array == 0) & (y_test_list[i - 1] == 0)).sum()\n",
    "            match_class_1 += ((predicted_array == 1) & (y_test_list[i - 1] == 1)).sum()\n",
    "\n",
    "            total_class_0 += (y_test_list[i - 1] == 0).sum()\n",
    "            total_class_1 += (y_test_list[i - 1] == 1).sum()\n",
    "\n",
    "# 各クラスの一致率を計算\n",
    "accuracy_class_0 = match_class_0 / total_class_0 if total_class_0 > 0 else 0\n",
    "accuracy_class_1 = match_class_1 / total_class_1 if total_class_1 > 0 else 0\n",
    "\n",
    "# 結果を出力\n",
    "print(f'(下降)クラス0の一致数: {match_class_0} / クラス0の総数: {total_class_0} / 一致率: {match_class_0}/{total_class_0} = {accuracy_class_0:.4f}')\n",
    "print(f'(上昇)クラス1の一致数: {match_class_1} / クラス1の総数: {total_class_1} / 一致率: {match_class_1}/{total_class_1} = {accuracy_class_1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再実験(0, 1, 2)\n",
    "# 学習用：3500, ファインチューニング用：750, 評価用；750\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データをロードする関数\n",
    "def load_csv_data(folder_path, num_files=5000):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(1, num_files + 1):\n",
    "        features_file = os.path.join(folder_path, f\"features_audio_{i}.csv\")\n",
    "        labels_file = os.path.join(folder_path, f\"labels_audio_{i}.csv\")\n",
    "\n",
    "        df_X = pd.read_csv(features_file)\n",
    "        df_y = pd.read_csv(labels_file)\n",
    "\n",
    "        X = torch.tensor(df_X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(df_y.values.flatten(), dtype=torch.long)\n",
    "\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "\n",
    "    return X_list, y_list\n",
    "\n",
    "# 特徴量とラベルのロード\n",
    "folder_path = 'prepared_features_data'\n",
    "X_list, y_list = load_csv_data(folder_path)\n",
    "\n",
    "# データを学習用、ファインチューニング用、評価用に分ける関数\n",
    "def split_data(X_list, y_list, train_size=0.7, val_size=0.15, random_state=42):\n",
    "    # 学習用データと残りを分割\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_list, y_list, test_size=(1 - train_size), random_state=random_state)\n",
    "\n",
    "    # 残りのデータをファインチューニング用と評価用に分割\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(0.5), random_state=random_state)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# 学習データ、ファインチューニングデータ、評価データに分割\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(X_list, y_list, train_size=3500/5000, val_size=750/5000)\n",
    "\n",
    "# LSTMモデル定義\n",
    "class LSTMAccentModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMAccentModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 自己教師あり学習用のモデルを定義\n",
    "class SelfSupervisedModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SelfSupervisedModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 自己教師あり学習モデルの初期化\n",
    "self_supervised_model = SelfSupervisedModel(input_size=5, hidden_size=64)\n",
    "\n",
    "# 自己教師あり学習のトレーニングループ\n",
    "def train_self_supervised_model(model, X_train, num_epochs=100, learning_rate=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs in X_train:\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, inputs)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"Self-supervised Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Self-supervised Learning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# 自己教師あり学習の実行\n",
    "train_self_supervised_model(self_supervised_model, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングのためのLSTMモデルの初期化\n",
    "model = LSTMAccentModel(input_size=5, hidden_size=56, output_size=3)\n",
    "\n",
    "# ファインチューニングの実行\n",
    "def fine_tune_model(model, X_train, y_train, num_epochs=100, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in zip(X_train, y_train):\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"Fine-tuning Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.title('Fine-tuning Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# ファインチューニングの実行\n",
    "fine_tune_model(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果の保存\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 保存するフォルダの作成\n",
    "output_dir = 'predicted_accents'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 評価関数の修正: 予測結果をCSVファイルに保存\n",
    "def save_predictions_to_csv(model, X_test, y_test, output_dir):\n",
    "    model.eval()  # 評価モードに切り替え\n",
    "\n",
    "    with torch.no_grad():  # 勾配計算を無効に\n",
    "        for i, inputs in enumerate(X_test):\n",
    "            inputs = inputs.unsqueeze(0)  # バッチ次元を追加\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 2)  # 最も高いスコアを持つクラスを選択\n",
    "            \n",
    "            # 予測結果をTensorからNumPy配列に変換\n",
    "            predicted_np = predicted.view(-1).numpy()\n",
    "            \n",
    "            # CSVファイルに保存\n",
    "            file_name = os.path.join(output_dir, f'predicted_accent_{i+1}.csv')\n",
    "            pd.DataFrame(predicted_np, columns=['Predicted Accent']).to_csv(file_name, index=False)\n",
    "\n",
    "# 予測結果を保存\n",
    "save_predictions_to_csv(model, X_test, y_test, output_dir)\n",
    "\n",
    "print(f\"Predicted accents saved in the '{output_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# フォルダのパスを指定\n",
    "predicted_dir = 'predicted_accents'\n",
    "\n",
    "# y_testをリストに変換する関数\n",
    "def load_y_test_tensor(y_test):\n",
    "    return [tensor.numpy() for tensor in y_test]  # TensorをNumPy配列に変換してリストに追加\n",
    "\n",
    "# y_testをリスト形式でロード\n",
    "y_test_list = load_y_test_tensor(y_test)\n",
    "\n",
    "# 一致数をカウントするための変数を初期化\n",
    "total_matches = 0\n",
    "total_samples = 0\n",
    "match_class_0 = 0\n",
    "match_class_1 = 0\n",
    "match_class_2 = 0\n",
    "\n",
    "# 各クラスの総数をカウントするための変数を初期化\n",
    "total_class_0 = 0\n",
    "total_class_1 = 0\n",
    "total_class_2 = 0\n",
    "\n",
    "# 予測結果をロードして一致をカウント\n",
    "for i in range(1, 1001):  # predicted_accent_1.csv から predicted_accent_1000.csv まで\n",
    "    file_name = os.path.join(predicted_dir, f'predicted_accent_{i}.csv')\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        # CSVファイルを読み込む\n",
    "        predicted_df = pd.read_csv(file_name)\n",
    "        predicted_array = predicted_df['Predicted Accent'].to_numpy()\n",
    "\n",
    "        # 一致数をカウント\n",
    "        if i <= len(y_test_list):  # y_testの範囲内であるか確認\n",
    "            total_matches += (predicted_array == y_test_list[i - 1]).sum()\n",
    "            total_samples += len(predicted_array)\n",
    "\n",
    "            # 各クラスの一致数と総数をカウント\n",
    "            match_class_0 += ((predicted_array == 0) & (y_test_list[i - 1] == 0)).sum()\n",
    "            match_class_1 += ((predicted_array == 1) & (y_test_list[i - 1] == 1)).sum()\n",
    "            match_class_2 += ((predicted_array == 2) & (y_test_list[i - 1] == 2)).sum()\n",
    "\n",
    "            total_class_0 += (y_test_list[i - 1] == 0).sum()\n",
    "            total_class_1 += (y_test_list[i - 1] == 1).sum()\n",
    "            total_class_2 += (y_test_list[i - 1] == 2).sum()\n",
    "\n",
    "# 各クラスの一致率を計算\n",
    "accuracy_class_0 = match_class_0 / total_class_0 if total_class_0 > 0 else 0\n",
    "accuracy_class_1 = match_class_1 / total_class_1 if total_class_1 > 0 else 0\n",
    "accuracy_class_2 = match_class_2 / total_class_2 if total_class_2 > 0 else 0\n",
    "\n",
    "# 結果を出力\n",
    "print(f'クラス0の一致数: {match_class_0} / クラス0の総数: {total_class_0} / 一致率: {match_class_0}/{total_class_0} = {accuracy_class_0:.4f}')\n",
    "print(f'クラス1の一致数: {match_class_1} / クラス1の総数: {total_class_1} / 一致率: {match_class_1}/{total_class_1} = {accuracy_class_1:.4f}')\n",
    "print(f'クラス2の一致数: {match_class_2} / クラス2の総数: {total_class_2} / 一致率: {match_class_2}/{total_class_2} = {accuracy_class_2:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sotuken",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

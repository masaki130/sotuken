{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mitani/.conda/envs/sotuken/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-06 01:39:56.955017: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-06 01:39:58.103365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-06 01:39:58.437150: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-06 01:39:58.835019: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-06 01:39:59.562188: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-06 01:40:02.988877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0856,  0.3027,  0.3669,  ...,  0.0771,  0.3301, -0.1210],\n",
      "         [ 0.3103,  0.9518,  0.1079,  ...,  0.2146,  0.1853, -0.5152],\n",
      "         [ 0.2058,  0.5051,  0.4948,  ...,  0.1341, -0.0217, -0.0406]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# BERTのトークナイザーとモデルをロード\n",
    "tokenizer = BertTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "model = BertModel.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "\n",
    "# 文字「ア」をトークン化\n",
    "inputs = tokenizer(\"ア\", return_tensors=\"pt\")\n",
    "\n",
    "# モデルを通して埋め込みを取得\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 出力の中の埋め込み（最後の隠れ層の出力）\n",
    "embedding = outputs.last_hidden_state\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mitani/.conda/envs/sotuken/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-09 16:31:24.689529: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-09 16:31:24.696826: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-09 16:31:24.705650: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-09 16:31:24.708306: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-09 16:31:24.715097: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-09 16:31:25.147968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'basic5000_HuBERT_features_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# 任意の個数のファイルを選択\u001b[39;00m\n\u001b[1;32m     35\u001b[0m num_files_to_process \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m  \u001b[38;5;66;03m# ここで読み込むファイル数を指定（例: 5）\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m selected_files \u001b[38;5;241m=\u001b[39m \u001b[43mload_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_files_to_process\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# CSVファイルを順番に処理\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m csv_file \u001b[38;5;129;01min\u001b[39;00m selected_files:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# CSVファイルを読み込む\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m, in \u001b[0;36mload_files\u001b[0;34m(num_files)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_files\u001b[39m(num_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# ディレクトリ内のCSVファイルを取得\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# ソート（昇順）して最初の num_files 個を選択\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     files\u001b[38;5;241m.\u001b[39msort()  \u001b[38;5;66;03m# ファイル名をソートして順番を決定\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'basic5000_HuBERT_features_csv'"
     ]
    }
   ],
   "source": [
    "# 768次元\n",
    "# BERTで文字のエンベディングを取得\n",
    "# 保存先；embeddings_output\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# トークナイザーとモデルのロード\n",
    "tokenizer = BertTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "model = BertModel.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "\n",
    "# 入力ディレクトリと出力ディレクトリ\n",
    "input_dir = 'basic5000_HuBERT_features_csv'     # HuBERTで抽出した特徴量F\n",
    "output_dir = 'embeddings_output'\n",
    "\n",
    "# 出力ディレクトリを作成\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 任意の個数を指定してファイルを読み込む関数\n",
    "def load_files(num_files=None):\n",
    "    # ディレクトリ内のCSVファイルを取得\n",
    "    files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    # ソート（昇順）して最初の num_files 個を選択\n",
    "    files.sort()  # ファイル名をソートして順番を決定\n",
    "    if num_files is None or num_files > len(files):\n",
    "        selected_files = files\n",
    "    else:\n",
    "        selected_files = files[:num_files]\n",
    "    \n",
    "    return selected_files\n",
    "\n",
    "# 任意の個数のファイルを選択\n",
    "num_files_to_process = 5000  # ここで読み込むファイル数を指定（例: 5）\n",
    "selected_files = load_files(num_files=num_files_to_process)\n",
    "\n",
    "# CSVファイルを順番に処理\n",
    "for csv_file in selected_files:\n",
    "    # CSVファイルを読み込む\n",
    "    file_path = os.path.join(input_dir, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Mora列を取り出し、エンベディングを計算\n",
    "    moras = df['Mora'].tolist()\n",
    "    \n",
    "    # エンベディング結果を保存するリスト\n",
    "    embeddings = []\n",
    "\n",
    "    for mora in moras:\n",
    "        # Moraをトークン化し、エンベディングを計算\n",
    "        inputs = tokenizer(mora, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = model(**inputs)\n",
    "        # BERTの出力の[CLS]トークンのベクトル（通常最初のトークン）を使用\n",
    "        embedding = outputs.last_hidden_state[0][0].detach().numpy()\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    # 新しいデータフレームを作成\n",
    "    embedding_df = pd.DataFrame({\n",
    "        'Mora': moras,\n",
    "        'Embedding': embeddings\n",
    "    })\n",
    "\n",
    "    # 出力ファイル名を設定\n",
    "    output_file = os.path.join(output_dir, f'embeddings_{csv_file}')\n",
    "    \n",
    "    # エンベディングをCSVファイルとして保存\n",
    "    embedding_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Processed {csv_file}, saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不必要\n",
    "# 768次元必要なところ、767次元しかない\n",
    "# HuBERTの特徴量の次元数が足りない所をゼロパディング\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 入力ディレクトリと出力ディレクトリ\n",
    "input_dir = \"basic5000_features_with_accent\"\n",
    "output_dir = \"padded_features\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ゼロパディングを用いて768次元に揃える関数\n",
    "def zero_pad_to_768(features, target_dim=768):\n",
    "    current_dim = len(features)\n",
    "    if current_dim < target_dim:\n",
    "        return np.pad(features, (0, target_dim - current_dim), mode=\"constant\")  # ゼロでパディング\n",
    "    return features[:target_dim]  # 768次元に超えている場合、切り捨て\n",
    "\n",
    "# 指定された数のファイルを処理\n",
    "def process_files(input_dir, output_dir, num_files):\n",
    "    files = sorted([f for f in os.listdir(input_dir) if f.endswith(\".csv\")])  # ソートして順序を固定\n",
    "    selected_files = files[:num_files]  # 上からnum_files個のファイルを選択\n",
    "\n",
    "    for file in selected_files:\n",
    "        input_path = os.path.join(input_dir, file)\n",
    "        output_path = os.path.join(output_dir, file)\n",
    "\n",
    "        # ファイルの読み込み\n",
    "        df = pd.read_csv(input_path)\n",
    "\n",
    "        # Mora列とアクセント列を取得\n",
    "        mora_column = df[\"Mora\"]\n",
    "        accent_column = df[\"Accent\"]\n",
    "\n",
    "        # 音響特徴量列（Feature_0 から Feature_767）を抽出\n",
    "        feature_columns = [f\"Feature_{i}\" for i in range(768)]  # Feature_0 から Feature_767, 768→500\n",
    "        features = df[feature_columns].values\n",
    "\n",
    "        # 各行の音響特徴量をゼロパディングで補完\n",
    "        padded_features = []\n",
    "        for row in features:\n",
    "            # 非NaN値を取り出してゼロパディング\n",
    "            row_no_nan = row[~np.isnan(row)]  # NaNを取り除く\n",
    "            padded_row = zero_pad_to_768(row_no_nan)  # ゼロパディング\n",
    "            padded_features.append(padded_row)\n",
    "\n",
    "        # データフレームに再構成\n",
    "        padded_df = pd.DataFrame(\n",
    "            padded_features, columns=feature_columns\n",
    "        )\n",
    "        padded_df.insert(0, \"Mora\", mora_column)  # Mora列を追加\n",
    "        padded_df[\"Accent\"] = accent_column  # Accent列を追加\n",
    "\n",
    "        # CSVとして保存\n",
    "        padded_df.to_csv(output_path, index=False)\n",
    "\n",
    "# 実行例: 入力ディレクトリ内の上から5ファイルを処理\n",
    "num_files_to_process = 5  # 任意のファイル数\n",
    "process_files(input_dir, output_dir, num_files_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignment_1559.csv の特徴量のカウント結果を feature_counts_output/alignment_1559.csv_feature_counts.csv に保存しました。\n",
      "alignment_0341.csv の特徴量のカウント結果を feature_counts_output/alignment_0341.csv_feature_counts.csv に保存しました。\n",
      "alignment_0633.csv の特徴量のカウント結果を feature_counts_output/alignment_0633.csv_feature_counts.csv に保存しました。\n",
      "alignment_4265.csv の特徴量のカウント結果を feature_counts_output/alignment_4265.csv_feature_counts.csv に保存しました。\n",
      "alignment_2959.csv の特徴量のカウント結果を feature_counts_output/alignment_2959.csv_feature_counts.csv に保存しました。\n"
     ]
    }
   ],
   "source": [
    "# 768次元だった\n",
    "# パディング出来てるかを確認するコード\n",
    "# 何次元かをカウント可能\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 入力ディレクトリ（padded_features）と出力ディレクトリ（output_dir）のパス\n",
    "# input_dir = \"sample_HuBERT_features_csv\"\n",
    "# output_dir = \"sample_HuBERT_features_csv\"\n",
    "# input_dir = \"padded_features\"\n",
    "input_dir = \"basic5000_features_with_accent\"\n",
    "output_dir = \"feature_counts_output\"  # 出力先ディレクトリ\n",
    "\n",
    "# 出力ディレクトリが存在しない場合は作成\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 任意のファイル数を指定（例: 最初の3ファイルを処理）\n",
    "num_files_to_process = 5  # ここで処理するファイルの数を指定します\n",
    "\n",
    "# 入力ディレクトリ内の全てのCSVファイルを取得\n",
    "all_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "\n",
    "# 処理するファイルをリストの先頭から指定された数だけ取得\n",
    "file_list = all_files[:num_files_to_process]\n",
    "\n",
    "# 各ファイルを処理\n",
    "for file_name in file_list:\n",
    "    # ファイルパスを作成\n",
    "    input_file = os.path.join(input_dir, file_name)\n",
    "\n",
    "    # ファイルが存在する場合のみ処理\n",
    "    if os.path.exists(input_file):\n",
    "        # CSVファイルを読み込み（ヘッダーを無視して2行目以降を読み込む）\n",
    "        df = pd.read_csv(input_file, header=0)\n",
    "\n",
    "        # 特徴量列の開始位置を特定 (Feature_0 ～ Feature_N)\n",
    "        feature_columns = [col for col in df.columns if col.startswith(\"Feature_\")]\n",
    "\n",
    "        # 各音節の特徴量の数をカウントする\n",
    "        df_counts = df[[\"Mora\"]].copy()  # Mora列をコピー\n",
    "        df_counts[\"Feature_Count\"] = df[feature_columns].notnull().sum(axis=1)  # 非NaNの特徴量の数をカウント\n",
    "\n",
    "        # 出力ファイルのパスを設定（ファイル名に対応した名前を付ける）\n",
    "        output_file = os.path.join(output_dir, f\"{file_name}_feature_counts.csv\")\n",
    "\n",
    "        # 結果を新しいCSVファイルに保存\n",
    "        df_counts.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"{file_name} の特徴量のカウント結果を {output_file} に保存しました。\")\n",
    "    else:\n",
    "        print(f\"{file_name} が見つかりません。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './embeddings_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m num_files_to_process \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# 任意のファイル数（例: 最初の5ファイルを処理）\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# 特徴量を変換して保存\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mconvert_embeddings_to_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_files_to_process\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m, in \u001b[0;36mconvert_embeddings_to_features\u001b[0;34m(input_dir, output_dir, num_files)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_embeddings_to_features\u001b[39m(input_dir, output_dir, num_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# 入力ディレクトリ内のすべてのファイルを取得\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m         files \u001b[38;5;241m=\u001b[39m files[:num_files]  \u001b[38;5;66;03m# 最初のnum_files個のファイルだけを処理\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './embeddings_output'"
     ]
    }
   ],
   "source": [
    "# embedding_outputがない！, features_karam生成\n",
    "# 文字数が合わない！\n",
    "# Mora,Feature_0,Feature_1,Feature_2,Feature_3,Feature_4,...,Feature_767と横一列にした\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 特徴量を展開してCSVに保存する関数\n",
    "def convert_embeddings_to_features(input_dir, output_dir, num_files=None):\n",
    "    # 入力ディレクトリ内のすべてのファイルを取得\n",
    "    files = [f for f in os.listdir(input_dir) if f.endswith(\".csv\")]\n",
    "    \n",
    "    if num_files is not None:\n",
    "        files = files[:num_files]  # 最初のnum_files個のファイルだけを処理\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(input_dir, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # 新しいデータフレームを作成\n",
    "        features = []\n",
    "\n",
    "        # 音節（Mora）とその特徴量を変換\n",
    "        for _, row in df.iterrows():\n",
    "            mora = row['Mora']\n",
    "            embedding_str = row['Embedding']\n",
    "            # numpy配列として変換\n",
    "            embedding = np.fromstring(embedding_str.strip('[]'), sep=' ', dtype=np.float32)\n",
    "\n",
    "            # 音節とその特徴量を新しい行として保存\n",
    "            features.append([mora] + embedding.tolist())\n",
    "\n",
    "        # 新しいデータフレームを作成\n",
    "        feature_columns = ['Mora'] + [f'MFeature_{i}' for i in range(embedding.shape[0])]\n",
    "        feature_df = pd.DataFrame(features, columns=feature_columns)\n",
    "\n",
    "        # 出力ファイルパス\n",
    "        output_file_path = os.path.join(output_dir, file)\n",
    "        feature_df.to_csv(output_file_path, index=False)\n",
    "        print(f\"Saved: {output_file_path}\")\n",
    "\n",
    "# 実行部分\n",
    "input_directory = \"./embeddings_output\"  # 特徴量が保存されているディレクトリ\n",
    "output_directory = \"./processed_embeddings_karam\"  # 処理後の特徴量を保存するディレクトリ\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "num_files_to_process = 3  # 任意のファイル数（例: 最初の5ファイルを処理）\n",
    "\n",
    "# 特徴量を変換して保存\n",
    "convert_embeddings_to_features(input_directory, output_directory, num_files=num_files_to_process)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sotuken",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
